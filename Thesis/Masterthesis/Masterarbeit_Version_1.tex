%%%%%%%%%%%%%%%%%%%%%%%% Title Page %%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt, a4paper, titlepage]{article}
\usepackage[a4paper,left=2.54cm,right=2.54cm,top=2.54cm,bottom=2.54cm]{geometry}

\usepackage{placeins}
\usepackage{float}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{appendix}
\usepackage{setspace}
\usepackage{dcolumn}
\usepackage[printonlyused, withpage]{acronym}
\usepackage{amsmath}
\usepackage{natbib}
\makeatletter
\newcommand{\MSonehalfspacing}{%
  \setstretch{1.44}%  default
  \ifcase \@ptsize \relax % 10pt
    \setstretch {1.448}%
  \or % 11pt
    \setstretch {1.399}%
  \or % 12pt
    \setstretch {1.433}%
  \fi
}
\newcommand{\MSdoublespacing}{%
  \setstretch {1.92}%  default
  \ifcase \@ptsize \relax % 10pt
    \setstretch {1.936}%
  \or % 11pt
    \setstretch {1.866}%
  \or % 12pt
    \setstretch {1.902}%
  \fi
}
\makeatother
\MSonehalfspacing


%%%%%%%%%%%%%%DOCUMENT%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%TITLEPAGE%%%%%%%%%%%%%%
\begin{titlepage}
    \begin{center}
    {\LARGE \textbf{Job Title Classification Strategies for the German Labor Market}}
    \\[1cm]
    {\Large \textbf{Masterthesis}}
    \\[1cm]
    {\Large submitted by}
    \\[0.5cm]
    {\LARGE \textbf{Rahkakavee Baskaran}}
    \\[0.5cm]
    {\Large at the}
    \\[0.5cm]
    \includegraphics[width=0.4\textwidth]{logo.jpg}
    \\[1cm]
    {\Large \textbf{Department of Politics and Public Administration}}
    \\[1cm]
    {\Large \textbf{Center for Data and Methods}}
    \\[2cm]
    \begin{minipage}[c]{0.8\textwidth}
    \begin{description}
     \item {\Large \textbf{1.Gutachter:} Prof. Dr. Susumu Shikano}
     \item {\Large \textbf{2.Gutachter:} JunProf Juhi Kulshresthra}
    \end{description}
    \end{minipage}
    \vfill
    {\LARGE \textbf{Konstanz, \today}}
    \end{center}
    \end{titlepage}

%%%%%%%%%%%%%%TableOfContents%%%%%%%%%%%%%%
\tableofcontents
\newpage


%%%%%%%%%%%%%%Abbreviations%%%%%%%%%%%%%%
\section*{Abbreviations}
\begin{acronym}
  \acro{SVM}[SVM]{Support Vector Machine}
  \acro{OA}[OA]{overall accuracy}
  \acro{ROC}[ROC]{receiver operating characteristics}
  \acro{TP}[TP]{True positives}
  \acro{TN}[TN]{True negatives}
  \acro{FN}[FN]{False negatives}
  \acro{FP}[FP]{False positives}
\end{acronym}
\newpage

%%%%%%%%%%%%%%SECTIONS%%%%%%%%%%%%%%
\section{Introduction}
\section{Related work}
\subsection{Short text classifcation}
\subsection{Multiclass classification}
\section{Data and taxonomy}
\section{Evaluation metrics}

There exists several metrics for the evaluation of classification approaches in the literature \citep{Fatourechi2008}. The choice of appropriate measurements is a crucial step for obtaining a qualitative comparison in the performance between the baseline algorithms and the new approaches. Often researchers rely on popular metrics like \ac{OA}. However, especially for multiclass and inbalanced dataset tasks it is difficult to rely only on one measure like \ac{OA} In order to select appropriate metrics for comparison in the following the most important metrics will be discussed focussing on multiclass classification and imbalanced data sets. 

Most metrics rely on a confusion matrix. For the multiclass case this confusion matrix is defined as follows \citep{Kautz2017}: 
\begin{table}[hb!]
  \center
  \begin{tabular}{lllll}
  \hline
            & positive examples      &           &             &             \\ \hline
  positive prediction  & $c_{1,1}$ & $c_{1,2}$ & $\dots$     & $c_{1,n}$   \\
            & $c_{2,1}$ & $c_{i,j}$ &             &             \\
            &  $\vdots$         &           & $\ddots$ &   $\vdots$\\
            & $c_{n,1}$ &           & $\dots$     & $c_{n,n}$   \\ \hline
  \end{tabular}
  \caption{\label{tab: T1} Confusion Matrix (edited after \citep[113]{Kautz2017}}
  \end{table}

From the confusion matrix follows that $c_{i, j}$ defines examples which belong to class j and are predicted as class i. Given that $k$ is the current class, \ac{TP} is defined as $tp_{k} = c_{k, k}$, thus examples which are correctly predicted as the current class k. \ac{FN} are defined as those examples which not belonging to the current class k, but are predicted as k. Formally $fn_{k} = \sum_{i=1, i \neq k}^n c_{i, k}$. Next, \ac{TN}, are examples belonging to the current class m, but are not predicted as m. Formally $tn_{k} = \sum_{i=1, i\neq k}^n \sum_{j=1, j \neq k}^n c_{i,j}$. Last, \ac{FP} are defined as examples not belonging to class k, but are predicted as such. Formally this can be expressed as: $fp_{k} =  \sum_{i=1, i \neq k}^n c_{k, i}$ \citep{Kautz2017}

As mentioned the \ac{OA} is one of most common metric for performance evaluation. It represents how well the classifier classifies across all classes correctly. Formally, given that N is number of examples and K the number of all classes, this can be expressed as \citep{Branco2017}: 
\[OA = \frac{1}{K} \sum_{i = 1}^K \frac{tp_{k} + tn_{k}}{N}\]
% \[OA = \frac{TP+ TN}{n}\]
Following the formula an accuracy of 1 means that all examples are correctly classified, while a 0 mean that each example is classified with the wrong class. \citep{Berthold2020}
Although \ac{OA} is a widley used metric it is critized for favouring the majority classes, thus not reflecting minority classes appropriatly in unbalanced datasets \citep{Berthold2020, Fatourechi2008}

Two more popular metrics are precision and recall. Precision respresents how well the classifier detects actual positive examples among the positive predicted examples. Recall, also called sensitivity, in contrast, represents how many examples are labelled as positive among the actual positive examples \citep{Berthold2020}. For the multiclass scenario, two different calculation approaches for each of the metrics are proposed: micro and macro average \citep{Branco2017}. In the macro approach first the metric is calculated for each class k against all other classes. The average of all of them is built. Formally: 

\[precision_{macro} = \frac{1}{K} \sum_{i=1}^k \frac{tp_{i}}{tp_{i} + fp_{i}}\]
\[recall_{macro} = \frac{1}{K} \sum_{i=1}^k \frac{tp_{i}}{tp_{i} + fn_{i}}\]

In contrast the micro approach aggregates the values, which can be formally expressed as follows: 

\[precision_{micro} = \frac{\sum_{i=1}^K tp_i}{\sum_{i=1}^K tp_i + fp_i}\]
\[recall_{micro} = \frac{\sum_{i=1}^K tp_i}{\sum_{i=1}^K tp_i + fn_i}\]

There is a trade-off between precision and recall \citep{Buckland1994}. The F-measure capture both precision and recall by taking the harmonic mean between both. It is calculated as follows \citep{Branco2017,Pan2016}:  

\[F_{micro} = 2 \cdot \frac{precision_{micro} \cdot recall_{micro} }{precision_{micro} + recall_{micro} }\ \]

\[F_{macro} = 2 \cdot \frac{precision_{macro} \cdot recall_{macro} }{precision_{macro} + recall_{macro} }\ \]

Apart from the trade-off between recall and precision, there is also a tradeoff between sensitivity and specificty (1- sensitivity). Using a \ac{ROC}, which plots the specifity against the sensitivity the trade-off can be visualized for different thresholds. The area under the curve then can be used to obtain the performance of the classifier. A large area indicates a better classifier \citep{Berthold2020, Espindola2005}. 

As shown above, there are several metrics for evaluating the performance of a classifier, with the metrics having different focuses. Since the job title classification involves multiclass classification and the descriptive analysis show that the data is clearly unbalanced, at least for some classes in level 5, it is not reasonable to base the evaluation solely on the \ac{OA}. Taking precision, recall and the harmonic mean into account would capture the performance of the minority classes as well. The \ac{ROC} curve does gives, due to its visualization a good impression for the performance, but it is not feasible for high number of classes. Following this argumentation the performance of the classifiers will be evaluated with accuracy, precision, recall, F-measure and Cohen's Kappa. 

\section{Baseline Algorithms}
The developed algorithm should be compared against the current state-of-the art methods in order to check the improvements 

CNN-ALGORITHMUS 

However, traditional methods like \ac{SVM} also performed well for text classification. Especially for multiclass tasks, as mentioned in the literature review, often different versions of the algorithm are used and showed good performance \citep{Aiolli2005,Angulo2003,Benabdeslem2006,Guo2015,Mayoraz1999,Tang2019,Tomar2015}. In general \ac{SVM} has several advantages for text classifcation. First, text classifcation usually has a high dimensional input space. \ac{SVM} can handle these large features since they are able to learn independently of the dimensionality of the feature space. In addition \ac{SVM}s are known to perform well for dense and sparse vectors, which is usually the case for text classification \citep{Joachims1998}. Empirical results, for example \citet{Joachims1998} or \cite{Liu2010} confirm the theoretical expectations. It is, therefore, a reasonable option to use a basic version of the \ac{SVM} algorithm as a baseline.

The general idea of a \ac{SVM} is to map ``the input vectors x into a high-dimensional feature space Z through some nonlinear mapping chosen a priori [...], where an optimal separating hyperplane is constructed'' \citep[138]{Vapnik2000}. In \ac{SVM} this optimal hyperplane maximizes the margin, which is simply put the distance from the hyperplane to the closest points, so called Support Vectors, across both classes \citep{Han2012}. Formally, given a training data set with n training vectors $x_i \in R^n, i = 1,....,n$ and the target classes $y_1,...y_i$ with $y_i \in \{-1, 1\}$, the following quadratic programming problem (primal) has to be solved in order to find the optimal hyperplane:
\[\min_{w,b} \frac{1}{2}w^{T}w \] 
\[\text{subject to } y_i(w^T\phi(x_i)+b) \geq 1\]

where $\phi(x_i)$ transforms $x_i$ into a higher dimensional space, $w$ corresponds to the weight and $b$ is the bias \citep{Chang2001,Jordan2006}
The given optimzation function assumes that the data can be separated without errors. This is not always possible, which is why \cite{Cortes1995} introduce a soft margin \ac{SVM}, which allows for missclassfication \citep{Vapnik2000}.
By adding a regularization parameter $C$ with $C > 0$ and the corresponding slack-variable $\xi$ the optimization problem changes to \citep{Chang2001, Han2012}: 
\[\min_{w,b} \frac{1}{2}w^{T}w + C \sum_{i=1}^n \xi_i \] 
\[\text{subject to } y_i(w^T\phi(x_i)+b) \geq 1 - \xi_i, \] 
\[\xi_i \geq, i = 1,...,n\]

Introducing Lagrange multipliers $\alpha_i$ and converting the above optimization problem into a dual problem the optimal $w$ meets \citep{Chang2001, Jordan2006}:
\[w = \sum_{I=1}^n y_i\alpha_i\phi(x_i)\]

with the decision function \citep{Chang2001}:
\[\text{sgn } (w^T\phi(x)+b) = sgn(\sum_{i=1}^n y_i \alpha K(x_i, x) +b)\]

$K(x_i, x)$ corresponds to a Kernel function, which allows to calculate the dot product in the original input space without knowing the exact mapping into the higher space \citep{Han2012, Jordan2006}. 

In order to apply \ac{SVM} to multiclass problems several approaches have been proposed. One stratetgy is to divide the multi-classifcation problem into several binary problems. A common approach here is the one-against-all method. In this method as many \ac{SVM} classifiers are constructed as there are classes k. The k-th classifier assumes that the examples with the k label are positive labels, while all the other examples treated as negative. Another popular approach is the one-against-one method. In this approach $k(k-1)/2$ classifiers are constructed allowing to train in each classifier the data of two classes \citep{Hsu2002}. Besides dividing the multiclass problem into several binary problems, some researches propose approaches to solve the task in one single optimization problem, like \citet{Crammer2001}. \footnote{For a detailed overview of all different methods and the method of \citet{Crammer2001} see \citet{Hsu2002,Crammer2001}}. 

In order to find a strong baseline I checked \ac{SVM}'s with different parameters for the \ac{SVM}, as well as different multiclass approaches. It appears that a SVM using a soft margin with a $C=1$ and a one-vs-rest approach has the best results. I also test different kernels, like  RBF Kernel or linear kernel. The linear kernel, formally  $k(x, x') = x^Tx'$, achieved the best results, which is why I choose it for the baseline. 

\section{Implementation of ...}

\section{Experimental results}
\section{Discussion and Limitations}


% \begin{tabular}{lrrrr}
%   \toprule
%   {} &  precision &    recall &  f1-score &       support \\
%   \midrule
%   1            &   0.888594 &  0.599284 &  0.715812 &    559.000000 \\
%   2            &   0.734189 &  0.858776 &  0.791611 &  13383.000000 \\
%   3            &   0.815764 &  0.658732 &  0.728886 &   3865.000000 \\
%   4            &   0.772832 &  0.776412 &  0.774618 &   5613.000000 \\
%   5            &   0.874323 &  0.793644 &  0.832032 &   7521.000000 \\
%   6            &   0.757537 &  0.689678 &  0.722017 &   6084.000000 \\
%   7            &   0.705632 &  0.757670 &  0.730726 &   8963.000000 \\
%   8            &   0.892236 &  0.839548 &  0.865090 &   5927.000000 \\
%   9            &   0.666250 &  0.521782 &  0.585232 &   2043.000000 \\
%   accuracy     &   0.773379 &  0.773379 &  0.773379 &      0.773379 \\
%   macro avg    &   0.789706 &  0.721725 &  0.749558 &  53958.000000 \\
%   weighted avg &   0.777862 &  0.773379 &  0.772496 &  53958.000000 \\
%   \bottomrule
%   \end{tabular}


% \begin{tabular}{lrrrr}
%   \toprule
%   {} &  precision &    recall &  f1-score &       support \\
%   \midrule
%   111          &   0.493333 &  0.411111 &  0.448485 &     90.000000 \\
%   112          &   0.580645 &  0.580645 &  0.580645 &     31.000000 \\
%   113          &   0.500000 &  0.384615 &  0.434783 &     13.000000 \\
%   114          &   1.000000 &  1.000000 &  1.000000 &      1.000000 \\
%   115          &   0.928571 &  0.722222 &  0.812500 &     18.000000 \\
%   116          &   1.000000 &  1.000000 &  1.000000 &      1.000000 \\
%   117          &   0.565217 &  0.325000 &  0.412698 &     40.000000 \\
%   121          &   0.746914 &  0.746914 &  0.746914 &    324.000000 \\
%   122          &   0.861111 &  0.861111 &  0.861111 &     36.000000 \\
%   211          &   0.105263 &  0.166667 &  0.129032 &     12.000000 \\
%   212          &   0.277778 &  0.253165 &  0.264901 &     79.000000 \\
%   213          &   0.428571 &  0.294118 &  0.348837 &     51.000000 \\
%   214          &   0.444444 &  0.320000 &  0.372093 &     50.000000 \\
%   221          &   0.440111 &  0.395000 &  0.416337 &    400.000000 \\
%   222          &   0.692810 &  0.658385 &  0.675159 &    161.000000 \\
%   223          &   0.549669 &  0.614815 &  0.580420 &    540.000000 \\
%   231          &   0.349650 &  0.198413 &  0.253165 &    252.000000 \\
%   232          &   0.590308 &  0.587719 &  0.589011 &    228.000000 \\
%   233          &   0.357143 &  0.625000 &  0.454545 &      8.000000 \\
%   234          &   0.586207 &  0.593023 &  0.589595 &     86.000000 \\
%   241          &   0.597015 &  0.465116 &  0.522876 &     86.000000 \\
%   242          &   0.603393 &  0.604369 &  0.603880 &   1236.000000 \\
%   243          &   0.394231 &  0.338843 &  0.364444 &    121.000000 \\
%   244          &   0.684211 &  0.603020 &  0.641055 &    927.000000 \\
%   245          &   0.604790 &  0.459091 &  0.521964 &    220.000000 \\
%   251          &   0.472149 &  0.500703 &  0.486007 &   2133.000000 \\
%   252          &   0.602469 &  0.538037 &  0.568433 &    907.000000 \\
%   261          &   0.458199 &  0.380507 &  0.415755 &    749.000000 \\
%   262          &   0.615728 &  0.606514 &  0.611086 &   1136.000000 \\
%   263          &   0.467678 &  0.435602 &  0.451071 &   1910.000000 \\
%   271          &   0.466667 &  0.486631 &  0.476440 &    374.000000 \\
%   272          &   0.639073 &  0.571006 &  0.603125 &    338.000000 \\
%   273          &   0.419622 &  0.421115 &  0.420367 &    843.000000 \\
%   281          &   0.228571 &  0.135593 &  0.170213 &     59.000000 \\
%   282          &   0.635135 &  0.431193 &  0.513661 &    109.000000 \\
%   283          &   0.600000 &  0.409091 &  0.486486 &     22.000000 \\
%   291          &   0.444444 &  0.500000 &  0.470588 &      8.000000 \\
%   292          &   0.536264 &  0.550790 &  0.543430 &    443.000000 \\
%   293          &   0.832680 &  0.845950 &  0.839262 &    753.000000 \\
%   311          &   0.551255 &  0.624408 &  0.585556 &    844.000000 \\
%   312          &   0.688525 &  0.700000 &  0.694215 &     60.000000 \\
%   321          &   0.651558 &  0.547619 &  0.595084 &    420.000000 \\
%   322          &   0.555556 &  0.504202 &  0.528634 &    238.000000 \\
%   331          &   0.756098 &  0.794872 &  0.775000 &     78.000000 \\
%   332          &   0.820946 &  0.718935 &  0.766562 &    338.000000 \\
%   333          &   0.501887 &  0.405488 &  0.448567 &    328.000000 \\
%   341          &   0.569024 &  0.528125 &  0.547812 &    320.000000 \\
%   342          &   0.703037 &  0.745823 &  0.723798 &    838.000000 \\
%   343          &   0.534328 &  0.339015 &  0.414832 &    528.000000 \\
%   411          &   0.382979 &  0.439024 &  0.409091 &     41.000000 \\
%   412          &   0.575949 &  0.461929 &  0.512676 &    197.000000 \\
%   413          &   0.560878 &  0.596603 &  0.578189 &    471.000000 \\
%   414          &   0.552239 &  0.362745 &  0.437870 &    102.000000 \\
%   421          &   0.388889 &  0.250000 &  0.304348 &     28.000000 \\
%   422          &   0.380952 &  0.222222 &  0.280702 &     36.000000 \\
%   423          &   0.189189 &  0.140000 &  0.160920 &     50.000000 \\
%   431          &   0.418046 &  0.295841 &  0.346484 &   1707.000000 \\
%   432          &   0.393443 &  0.451883 &  0.420643 &    478.000000 \\
%   433          &   0.467692 &  0.460606 &  0.464122 &   1320.000000 \\
%   434          &   0.575949 &  0.695719 &  0.630194 &   1308.000000 \\
%   511          &   0.260870 &  0.222222 &  0.240000 &     27.000000 \\
%   512          &   0.625000 &  0.512821 &  0.563380 &     39.000000 \\
%   513          &   0.651816 &  0.785527 &  0.712452 &   3427.000000 \\
%   514          &   0.153846 &  0.083333 &  0.108108 &     24.000000 \\
%   515          &   0.288462 &  0.230769 &  0.256410 &     65.000000 \\
%   516          &   0.531332 &  0.508750 &  0.519796 &    800.000000 \\
%   521          &   0.847046 &  0.886313 &  0.866235 &    906.000000 \\
%   522          &   0.684211 &  0.565217 &  0.619048 &     23.000000 \\
%   523          &   0.000000 &  0.000000 &  0.000000 &      2.000000 \\
%   524          &   1.000000 &  0.444444 &  0.615385 &     18.000000 \\
%   525          &   0.822222 &  0.789100 &  0.805320 &    844.000000 \\
%   531          &   0.751740 &  0.682105 &  0.715232 &    475.000000 \\
%   532          &   0.000000 &  0.000000 &  0.000000 &      4.000000 \\
%   533          &   0.388889 &  0.269231 &  0.318182 &     26.000000 \\
%   541          &   0.868897 &  0.857270 &  0.863044 &   1121.000000 \\
%   611          &   0.529505 &  0.618462 &  0.570536 &   1625.000000 \\
%   612          &   0.378788 &  0.152439 &  0.217391 &    328.000000 \\
%   613          &   0.552408 &  0.623003 &  0.585586 &    313.000000 \\
%   621          &   0.638641 &  0.683959 &  0.660524 &   1677.000000 \\
%   622          &   0.545181 &  0.419954 &  0.474443 &    431.000000 \\
%   623          &   0.645000 &  0.535270 &  0.585034 &    241.000000 \\
%   624          &   0.757576 &  0.568182 &  0.649351 &     44.000000 \\
%   625          &   0.200000 &  0.166667 &  0.181818 &      6.000000 \\
%   631          &   0.590909 &  0.428571 &  0.496815 &     91.000000 \\
%   632          &   0.650763 &  0.704545 &  0.676587 &    484.000000 \\
%   633          &   0.693950 &  0.755814 &  0.723562 &    774.000000 \\
%   634          &   0.532710 &  0.527778 &  0.530233 &    108.000000 \\
%   711          &   0.469388 &  0.216981 &  0.296774 &    106.000000 \\
%   712          &   0.500000 &  0.200000 &  0.285714 &      5.000000 \\
%   713          &   0.436769 &  0.364312 &  0.397264 &   2152.000000 \\
%   714          &   0.534190 &  0.614611 &  0.571586 &   3381.000000 \\
%   715          &   0.657996 &  0.774376 &  0.711458 &    882.000000 \\
%   721          &   0.619403 &  0.481625 &  0.541893 &    517.000000 \\
%   722          &   0.672783 &  0.792079 &  0.727573 &   1111.000000 \\
%   723          &   0.712644 &  0.738095 &  0.725146 &    252.000000 \\
%   731          &   0.752809 &  0.703412 &  0.727273 &    381.000000 \\
%   732          &   0.483705 &  0.402282 &  0.439252 &    701.000000 \\
%   733          &   0.590164 &  0.433735 &  0.500000 &     83.000000 \\
%   811          &   0.803625 &  0.807284 &  0.805450 &    659.000000 \\
%   812          &   0.726415 &  0.658120 &  0.690583 &    117.000000 \\
%   813          &   0.755864 &  0.657090 &  0.703024 &   1079.000000 \\
%   814          &   0.907463 &  0.907463 &  0.907463 &    335.000000 \\
%   815          &   0.777778 &  0.583333 &  0.666667 &     12.000000 \\
%   816          &   0.791045 &  0.582418 &  0.670886 &     91.000000 \\
%   817          &   0.852055 &  0.847411 &  0.849727 &    367.000000 \\
%   818          &   0.613793 &  0.468421 &  0.531343 &    190.000000 \\
%   821          &   0.659596 &  0.788647 &  0.718372 &    828.000000 \\
%   822          &   0.400000 &  0.222222 &  0.285714 &     63.000000 \\
%   823          &   0.910000 &  0.862559 &  0.885645 &    211.000000 \\
%   824          &   0.169492 &  0.714286 &  0.273973 &     14.000000 \\
%   825          &   0.682635 &  0.686747 &  0.684685 &    166.000000 \\
%   831          &   0.692469 &  0.828190 &  0.754273 &   1199.000000 \\
%   832          &   0.804255 &  0.697417 &  0.747036 &    271.000000 \\
%   833          &   0.000000 &  0.000000 &  0.000000 &      6.000000 \\
%   841          &   0.576923 &  0.416667 &  0.483871 &     72.000000 \\
%   842          &   0.508197 &  0.500000 &  0.504065 &    124.000000 \\
%   843          &   0.300813 &  0.366337 &  0.330357 &    101.000000 \\
%   844          &   0.573034 &  0.536842 &  0.554348 &     95.000000 \\
%   845          &   0.588235 &  0.588235 &  0.588235 &     51.000000 \\
%   911          &   0.400000 &  0.200000 &  0.266667 &     10.000000 \\
%   912          &   0.142857 &  0.038462 &  0.060606 &     26.000000 \\
%   913          &   0.455285 &  0.290155 &  0.354430 &    193.000000 \\
%   914          &   0.750000 &  0.375000 &  0.500000 &      8.000000 \\
%   921          &   0.545122 &  0.642241 &  0.589710 &   1392.000000 \\
%   922          &   0.415094 &  0.431373 &  0.423077 &     51.000000 \\
%   923          &   0.307692 &  0.133333 &  0.186047 &     30.000000 \\
%   924          &   0.580460 &  0.753731 &  0.655844 &    134.000000 \\
%   931          &   0.250000 &  0.100000 &  0.142857 &     20.000000 \\
%   932          &   0.750000 &  0.631579 &  0.685714 &     38.000000 \\
%   933          &   0.000000 &  0.000000 &  0.000000 &      8.000000 \\
%   934          &   0.000000 &  0.000000 &  0.000000 &      4.000000 \\
%   935          &   0.521739 &  0.500000 &  0.510638 &     24.000000 \\
%   936          &   0.200000 &  0.333333 &  0.250000 &      3.000000 \\
%   941          &   1.000000 &  0.666667 &  0.800000 &      3.000000 \\
%   942          &   0.000000 &  0.000000 &  0.000000 &      1.000000 \\
%   943          &   0.000000 &  0.000000 &  0.000000 &      4.000000 \\
%   944          &   0.076923 &  0.037037 &  0.050000 &     27.000000 \\
%   945          &   0.520833 &  0.490196 &  0.505051 &     51.000000 \\
%   946          &   0.333333 &  0.100000 &  0.153846 &     10.000000 \\
%   947          &   1.000000 &  0.333333 &  0.500000 &      6.000000 \\
%   accuracy     &   0.597829 &  0.597829 &  0.597829 &      0.597829 \\
%   macro avg    &   0.538437 &  0.484468 &  0.500388 &  56113.000000 \\
%   weighted avg &   0.592700 &  0.597829 &  0.591416 &  56113.000000 \\
%   \bottomrule
%   \end{tabular}

\clearpage

\bibliographystyle{apalike}
\bibliography{export}
    
\end{document}