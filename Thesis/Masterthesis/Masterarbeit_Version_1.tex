%%%%%%%%%%%%%%%%%%%%%%%% Title Page %%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt, a4paper, titlepage]{article}
\usepackage[a4paper,left=3.0cm,right=3.0cm,top=2.54cm,bottom=2.54cm]{geometry}

\usepackage{placeins}
\usepackage{float}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{appendix}
\usepackage{setspace}
\usepackage{dcolumn}
\usepackage[printonlyused, withpage]{acronym}
\usepackage{amsmath}
\usepackage{xcolor}
\DeclareMathOperator*{\argmax}{arg\,max} 
\usepackage{natbib}
\makeatletter
\newcommand{\MSonehalfspacing}{%
  \setstretch{1.44}%  default
  \ifcase \@ptsize \relax % 10pt
    \setstretch {1.448}%
  \or % 11pt
    \setstretch {1.399}%
  \or % 12pt
    \setstretch {1.433}%
  \fi
}
\newcommand{\MSdoublespacing}{%
  \setstretch {1.92}%  default
  \ifcase \@ptsize \relax % 10pt
    \setstretch {1.936}%
  \or % 11pt
    \setstretch {1.866}%
  \or % 12pt
    \setstretch {1.902}%
  \fi
}
\makeatother
\MSonehalfspacing

\usepackage{listings}
\usepackage{xcolor}

\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily,
    numbers=left,
    numberstyle=\scriptsize,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    frame=lines,
    backgroundcolor=\color{background},
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}

\usepackage{caption}
\usepackage{subcaption}

%%%%%%%%%%%%%%DOCUMENT%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%TITLEPAGE%%%%%%%%%%%%%%
\begin{titlepage}
    \begin{center}
    {\LARGE \textbf{Job title classification - A comparision of vectorization and classification techniques for German job postings}}
    \\[1cm]
    {\Large \textbf{Masterthesis}}
    \\[1cm]
    {\Large submitted by}
    \\[0.5cm]
    {\LARGE \textbf{Rahkakavee Baskaran}}
    \\[0.5cm]
    {\Large at the}
    \\[0.5cm]
    \includegraphics[width=0.4\textwidth]{logo.jpg}
    \\[1cm]
    {\Large \textbf{Department of Politics and Public Administration}}
    \\[1cm]
    {\Large \textbf{Center for Data and Methods}}
    \\[2cm]
    \begin{minipage}[c]{0.8\textwidth}
    \begin{description}
     \item {\Large \textbf{1.Gutachter:} Prof. Dr. Susumu Shikano}
     \item {\Large \textbf{2.Gutachter:} JunProf Juhi Kulshresthra}
    \end{description}
    \end{minipage}
    \vfill
    {\LARGE \textbf{Konstanz, \today}}
    \end{center}
    \end{titlepage}

%%%%%%%%%%%%%%TableOfContents%%%%%%%%%%%%%%
\tableofcontents
\newpage
\listoffigures
\newpage
\listoftables
\newpage
%%%%%%%%%%%%%%Abbreviations%%%%%%%%%%%%%%
\section*{Abbreviations}
\begin{acronym}
  \acro{SVM}[SVM]{Support Vector Machine}
  \acro{KNN}[KNN]{k-nearest-neighbor}
  \acro{NB}[NB]{Naive Bayes}
  \acro{MLR}[MLR]{Multinomial Logistic Regression}
  \acro{OA}[OA]{overall accuracy}
  \acro{ROC}[ROC]{receiver operating characteristics}
  \acro{TP}[TP]{True positives}
  \acro{TN}[TN]{True negatives}
  \acro{FN}[FN]{False negatives}
  \acro{FP}[FP]{False positives}
  \acro{MAP}[MAP]{maximum a posteriori}
  \acro{KldB}[KldB]{Klassifikation der Berufe 2010}
  \acro{ISCO}[ISCO]{International Standard Clasification of Occupations}
  \acro{BOW}[BOW]{Bag of Words}
  \acro{TF-IDF}[TF-IDF]{Term Frequency - Inverse Document Frequency}
  \acro{TF}[TF]{Term Frequency}
  \acro{IDF}[IDF]{Inverse Document Frequency}
  \acro{DF}[DF]{Document Frequency}
  \acro{CBOW}[CBOW]{continous bag of words}
  \acro{BERT}[BERT]{Bidirectional Encoder  Representations from Transformers}
  \acro{NLP}[NLP]{Nature Language Processing}
  \acro{PCA}[PCA]{Principal Component Analysis}
  \acro{RF}[RF]{Random Forest}
  \acro{CART}[CART]{Classification and Regression Trees}
  \acro{CNN}[CNN]{Convolutional Neural Network}
  \acro{KNN}[KNN]{K-nearest neighbors}
  \acro{LR}[LR]{Logistic Regression}
  \acro{RNN}[RNN]{Recurrent Neural Networks}
\end{acronym}
\newpage

%%%%%%%%%%%%%%SECTIONS%%%%%%%%%%%%%%
\section{Introduction}
Job titles are key information within the labor market. They are useful for job seekers to find jobs \citep{marinescu2020}. They are an important component of job search engines \citep{slamet2018, javed2015, javed2016} and job recommendation systems \citep{malherbe2014}. 

And lastly, they serve as a valuable data source for various analyses, such as job market trend \citep{martin2021, li2021}, job perception \citep{smith1989, boydston2020} or social science analyses \citep{martin2021}. However, since job titles are not normalized, it is challenging to structure them in an appropriate way for downstream tasks. Various institutions developed job taxonomies in order to structure and generalize job titles. Established taxonomies are, for example, the ``International Standard Classification of Occupation'' (ISCO) for the European job market or the ``Klassifikation der Berufe 2010'' (KLdB) for the German job market \citep{uter2020}. Matching job titles from job postings with classes from those taxonomies is inevitable in order to improve job search engines or recommendation systems as well as analyzing the labor market. In Natural Language Processing (NLP), this process of matching is known as text classification. 

Text classification, a highly researched area, is the process of classifying text documents or text segments into a set of predefined classes. 


\section{Related work}
\subsection*{Domain related work}
As being a useful application for many downstream task, some works for the English-language job market that deal with job classification can be found. In terms of classifiers, the corresponding work can be categorized into traditional classifiers or deep learning methods. \cite{zhu2017} for example, use a \ac{KNN} classifier in combination with document embedding as a feature selection strategy. \cite{javed2015} rely on traditional methods as well, by combining a \ac{SVM} classifier and a \ac{KNN} classifier for their job recommendation system. In contrast, the approaches of \cite{decorte2021}, \cite{wang2019} and \cite{neculoiu2016} are based on deep learning methods. From a higher perspective, there is another dividing line between the approaches. As mentioned earlier, job title normalization can be considered as a typical text classification task \citep{wang2019, javed2015, zhu2017}. \cite{decorte2021} and \cite{neculoiu2016}, however, formulate the task as a string representation approach of similar job titles. 

While there is some work on job title classification for the English speaking job market, as far as I am concerned, there have not been any classification attempts for the German job market. However, an accurate classification of job titles with the German taxonomy would facilitate several downstream tasks for the German job market. With the KldB 2010, an occupational classification was created for Germany that reflects the current trends in the labor market based on empirical and theoretical foundations. Further, most of the work about job title classification suffers from solid databases. Therefore, \cite{decorte2021}, for example, use skills to understand the meaning of job titles and to avoid manually labelling them. \cite{javed2015} rely on a weakly supervised approach to get enough labelled data. The advantage of classifying for the German job market is that the Federal Employment Agency of Germany provides a data set with job titles and the possibility of linking them with die KldB classes, which offers a huge and powerful training data set which in turn allows for more flexibility in which algorithms are applicable. Following \citet{wang2019, javed2015, zhu2017} the task will be framed as a text classification task. 

\subsection*{Textclassification}
During the last decades, researchers developed a various number of classifiers. As \cite{kowsari2019text} summarize in their survey of classifiers, we can group the approaches mainly into three groups. The first group contains traditional methods like \ac{NB},\ac{SVM}), \ac{KNN}, \ac{LR} or Decision Trees \citep{Vijayan2017, Colas2006, kowsari2019text, Sebastiani2001}. Deep learning methods like \ac{CNN} or \ac{RNN}, which are currently the most advanced algorithms for NLP, form the second group. The last group consists of ensemble learning techniques like Boosting and Bagging. Each group can be further split mainly into supervised and unsupervised learning techniques. Since labeled data is available for the classification task in this study, the focus will be on supervised learning techniques. 

Classification algorithms can be selected on different criteria. Certainly, one of the most important criteria is performance. Currently, deep learning methods often outperform traditional methods and ensemble techniques. Deep Learning methods are advanced methods of traditional machine learning algorithms. In contrast to traditional methods, they do not require a substantive understanding of feature extraction since they automatically extract important features. Many comparative studies on traditional and embedding techniques vs. deep learning text classification tasks show the strength of deep learning. \citet{wang2017} compared \ac{SVM} and \ac{KNN} for web text classification against \ac{CNN}. His results reveal a better performance of \ac{CNN} over the traditional methods. \citet{hassan2017} likewise shows that \ac{CNN}, but also \ac{RNN} outperforms traditional methods with \ac{BOW} feature extraction. These results are followed by \citet{kamath2018}. Furthermore \citet{gonzalez2020} compared the current state-of-the-art deep learning model \ac{BERT} with traditional methods using \ac{TF-IDF} feature selection method and shows clear outperformance by \ac{BERT}. Although deep learning models often outperform traditional methods in these comparative studies, not all classifiers have good results overall applications. In contrast to traditional methods, deep learning models also usually require millions of data to train an effective model \citep{chauhan2018}. Thus deep learning methods are not necessarily always the right choice. For example, \citet{zhang2015}, conducted experiments on character-level \ac{CNN} and compared them to different traditional models, like \ac{BOW} or ``bag-of-ngrams'' with \ac{TF-IDF} and \ac{LR}. For the smaller and moderate size news datasets, the traditional methods except for ``bag-of-means" performed well, and some of them outperformed \ac{CNN}. For bigger datasets, \ac{CNN} worked better. Another study from \citet{yan2018} rely on a siamese \ac{CNN} deep learning approach with few-shot learning for short text classification using different Twitter benchmark data. He compared his results to some baseline deep learning methods and traditional methods, among all \ac{SVM}, ensemble techniques, and \ac{LR}. Although the siamese \ac{CNN} outperformed all the other methods clearly, some traditional methods outperformed the baseline deep learning methods for specific datasets.  

Comparisons of ensemble technique, traditional methods, and methods within the traditional methods indicate that not all classifications perform equally good or poor for all tasks. A comparative analysis with \ac{LR}, \ac{RF} and k-nearest neighbor, using \ac{TF-IDF}, for news text classification indicates a good performance of \ac{LR} and \ac{RF} compared to \ac{KNN}, wherby \ac{LR} performed better then \ac{RF} \citep{shah2020}. A study from biomedical classfication shows best performance of \ac{SVM} among \ac{RF}, \ac{NB} with \ac{TF-IDF} \citep{danso2014}. 

Although performance is essential, other criteria like the transparency of the algorithm, the interpretability, and efficiency in terms of the runtime are not irrelevant. Methods like \ac{NB} or \ac{LR} are much faster than neural networks or \ac{SVM}s. In terms of transparency and interpretability, algorithms like decision tree or \ac{LR} are more intuitively easier to understand and interpret. In contrast, deep learning models and \ac{SVM} rely on more complex computations. Especially deep learning lacks transparency \citep{maglogiannis2007}. Considering \ac{BERT} although it usually outperforms other machine learning algorithms for text classification, in general ``there are more questions than answers about how \ac{BERT} works" \citep[853]{rogers2020}. It is, for example, not well-understood so far what exactly happens during the fine-tuning process of \ac{BERT} \citep{merchant2020}. However, the main focus of this analysis is on performance. 

Besides the classifier, it is also important for the performance with which inputs the classifiers are fed. Texts must be converted into a numerical representation to make them machine-readable \citep{singh2019}. The numerical vector representations of a text or document can be divided into sparse and dense vectors, also called word embeddings. Sparse vectors, relying on the \ac{BOW} model, are high-dimensional vectors with many zeros, while word embeddings techniques have a fixed-length representation \citep{almeida2019}. Sparse vectors are for example \ac{TF-IDF} or count vectorizer. Examples for word embedding techniques are word2vec, doc2vec, and \ac{BERT}. 

Unsuitable features considerably affect the performance of the classification algorithms \citep{cahyani2021}. The correct selection of a feature extraction technique depends on many factors, like the length of the dataset or the specific domain \citep{arora2021}. Empirically, this is reflected in the diverse and conflicting studies in the literature. Considering the sparse vectors, \citet{wendland2021} compared for fake news data \ac{TF-IDF} and count vectorization and found slightly better results with \ac{TF-IDF} while the results of \citet{WangY2017} show no difference between them. Some studies from different domains demonstrate the strength of word2vec comparing to \ac{TF-IDF} \citep{arora2021, rahmawati2016}, while others show opposite results \citep{zhu2016,cahyani2021}. \citet{shao2018} conclude that they find no clear picture between \ac{BOW} models and word2vec. Comparing doc2vec and word2vec \citet{lau2016} found in general good performance of doc2vec. However, the authors admit that the qualitative differences between both techniques are not clear. Both \citet{shao2018} and \citet{WangY2017} obtain the worst results for doc2vec compared to word2vec and \ac{BOW} vectorization techniques. \ac{BERT} shows overall a good performance \citep{gonzalez2020}. Nevertheless, \citet{miaschi2020} reach in their study the conclusion that word2vec and \ac{BERT} code similarly for sentence-related linguistic features.  

\subsection*{Challenges of job title classification}
Each text classification task has different challenges. One challenge of the presented task is the number of classes. As \cite{Li2004} shows in their classification of tissues, multiclass classification is more complex than binary classification problems. Partly because most classification algorithms were designed for binary problems \citep{Aly2005}. Approaches for multiclassification can be grouped into two types. Either binary algorithms can handle multiclassification naturally, or the problem is decomposed into binary classification tasks (for the different subtypes, see \cite{Aly2005}). The literature so far does not have a clear answer to solve multiclassification problems. Different approaches, like boosting \citep{Schapire2000} or \ac{CNN} \citep{Farooq2017} are applied. It is noticeable, however, that many works use variations of SVM \citep{Guo2015, Tomar2015, Tang2019}.  

Another important issue is the length of input documents for classification. Job titles are short text with often not more than 50 characters. Short texts suffer from sparseness, few word co-occurrences, missing shared context, noisiness, and ambiguity. These attributes make it challenging to construct features that capture the meaning of the text on the one side. On the other side, traditional methods are based on word frequency, high word co-occurrence, and context, which is why they often fail to achieve high accuracy for short texts \citep{Song2014, WangY2017, WangF2014,  alsmadi2019}. Besides this, short texts are also often characterized as having a lot of misspelling and informal writing. In addition, applications like Twitter deliver and process short texts in real-time. The last three attributes of short texts are indeed a problem, e.g., for Twitter data, which is a popular topic for short text data \citep{karimi2013, sriram2010, yan2018}. However, these attributes play little or no role in the job title classification, especially compared to the other stated issues, since job postings are usually reviewed and controlled thoroughly before release. Due to this reason, only research concerning the first mentioned attributes is considered in the following. 

A popular approach proposed by many researchers for short text classification is to add additional knowledge to the features to improve short text classification. Many studies of Twitter short texts demonstrate the power of this approach. \citet{karimi2013}, for example, use a \ac{BOW} approach for disaster classification of Twitter posts. They experiment with different features enriched by additional information. While, for example, generic features like the number of hashtags improved classification, other information like incident-specific features only helped in specific settings. All in all, the use of \ac{BOW} with specific features delivers quite good performance. Similarly, \citet{sriram2010} achieved as well with thoroughly manual extracted features from the short text good performance for Twitter short text messages. 

\citep{WangF2014} criticize the \ac{BOW} approach for short text classification since \ac{BOW} results usually in high dimensional data. They state that this is much more harmful to short texts because they are short and sparse. They propose a ``bag of concept" approach using a knowledge base. The knowledge base is used to learn concepts for each category and find a set of relevant concepts for each short text. Following this, \citep{wang2017J} use as well an enriching concept for short text classification. They use a concept vector with the help of a taxonomy knowledgebase which indicates how much a text is related to the concept. Those are merged with the word embeddings. In addition, they add character-level features. 

In general, in short text classification, the question arises whether to represent the features as dense or sparse vectors. In their comparison of \ac{TF-IDF} and count vectorizer against the dense vectorizer word2vec and doc2vec, \cite{WangY2017} conclude that among the classifiers \ac{NB}, \ac{LR} and \ac{SVM}, the sparse vectorizers achieve the highest accuracy. \cite{Chen2019}, conversely, see limitations in sparse representation as it cannot capture the context information. In their work, they integrate sparse and dense representation into a deep neural network with knowledge-powered attention, which outperforms state-of-art deep learning methods, like \ac{CNN}, for Chinese short texts. 

\citet{sun2012} pursues in contrast to the mentioned approaches a completely different strategy. Instead of enriching the features, he focused the features on specific keywords. In order to select the essential keywords, he used \ac{TF-IDF} approaches, some with a clarity function for each word. Using \ac{LR} he got pretty good results for his classification.

Instead of feature enrichment according to \citet{Song2014}, some researchers also apply feature dimensionality reduction and extraction of semantic relationship techniques using, for example, Latent Dirichlet approaches.

Concerning the classifiers, there is no consensus approach for short text classification. For traditional approaches, \cite{WangY2017}'s results indicate that \ac{LR} and \ac{SVM} perform best, while k-nearest neighbor seems to achieve the best accuracy in \cite{Khamar2013}'s work. \citet{Song2014} proposes to use ensemble techniques. In combination with enriched features \citet{Bouaziz2014}, for example, achieve better results as for \ac{LR} with ensemble techniques. Similar to job title-specific work, more recent work prefers deep learning methods, mostly \ac{CNN} \citep{Chen2019}. 


\subsection*{Implications}
There are three emerging consequences from the above-discussed literature. First, appropriate feature selection or vectorization plays a decisive role in the performance. For that reason, I implement several feature extraction techniques covering sparse and dense vectors. For sparse vectors, I transform the data with count vectorizer and \ac{TF-IDF} vectorizer. Word2vec, doc2vec, and \ac{BERT} embedding built the group of dense vectorization techniques. The discussion of the different techniques will be the first pillar of the comparison. 

Second, relying on one classification does not seem a reasonable option. Instead, experimenting and exploring different traditional, deep learning, and ensemble classifiers allow for identifying the best classifier based on the task and the data \citep{maglogiannis2007}. Therefore I use four classifier techniques. I fall back to two traditional methods. The literature shows that \ac{SVM} and \ac{LR} are well compatible with other techniques, which is why I choose both of them. I also include \ac{RF} as an ensemble technique. As the last method, I implement a \ac{BERT} deep learning model since it is the state-of-Art method currently for text classification. The evaluation of different classifications built the second pillar of the comparison. 

Third, from the two challenges presented, the focus is on the classification of short texts. The literature on short text classification reveals two points. There are different results on whether sparse or dense techniques are better suited. Testing different sparse and dense vectorization techniques allows covering for that point. Second, most of the solutions include additional knowledge. Therefore, I introduce a second model for each word2vec and doc2vec with additional knowledge from the taxonomy. The model comparison between these models built the last pillar of the comparison. 


\section{Job title data and taxonomy}
The training data consists of two data sets. \footnote{Both datasets are provided for the study by cause\&effect DFSG UG. They are not publicly available in this format} The classes are extracted from the first data set, referred to below as the \ac{KldB} dataset. The \ac{KldB} dataset contains all information of the \ac{KldB} taxonomy. The second dataset, called the job title dataset, contains the necessary data from the job titles. In the first part of this chapter, a brief explanation about the taxonomy structure and both datasets are given. The second part consists of a descriptive analysis of the class distribution of the data. 

\subsection{KldB 2010 Taxonomy}
The ``\ac{KldB}" dataset is structured hierarchically with five levels, with each level containing a different number of classes. The classifiers are trained for level 1 and level 3. In the following, these classes are also referred to as ``kldbs". On level 1, each class has an id of length 1 with a number from 0 to 9. Table \ref{tab: T3} shows the ten classes of level 1 with their class names. On level 2, each of the ten classes is divided into one or more subclasses having a class id of length 2, with the first digit indicating the class of level 1 and the second digit the class of level 2. An overview of all five levels with an example of classes is given in table \ref{tab: T2}. Note that the example in table \ref{tab: T2} does not show on level 2 to level 5 all classes. Thus on level 2, there exists also, e.g., the class id 41 with ``Mathematik-, Biologie- Chemie- und Physikberufe", which in turn is divided into other classes on level 3. This procedure ultimately leads to class ids of length five on level 5. An occupation can be classified on every level in the taxonomy. Considering the classes of the example in table \ref{tab: T2}, the job title ``Java Developer" could be classified on level 5 to the class 43412. From this id, it is also derivable that the job title belongs, for example, on level 3 to the class ``Sofwareentwicklung" \citep{Bundesagentur2011a, Bundesagentur2011b, Paulus2013}

The \ac{KldB} contains two dimensions. The first dimension, the so-called ``Berufsfachlichkeit", structures jobs according to their similarity in knowledge, activities, and jobs, reflected in the first four levels. Considering the example above and the job title ``Fullstack PHP-Entwickler". It is reasonable to classify both on level 1 to "Naturwissenschaft, Geografie and Information" because they are related to computer science. It also makes sense to classify them, for example, to 4341 because both are about software development. On level 5, then, a second dimension is introduced. the "Anforderungsniveau". This dimension gives information on the level of requirement for a job and four possible requirements. In table \ref{tab: T4} they are summarized. From the class id of job title ``Java Developer", we can see that the job has been assigned to the second requirement level since the last digit is a two \citep{Bundesagentur2011a, Bundesagentur2011b, Paulus2013}. 


\begin{table}[]
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{cl}
  \hline
  \multicolumn{1}{l}{\textbf{IDs KldB 2010}} & \textbf{Berufsbereich (Level 1)}                                           \\ \hline
  1                                          & Land-, Forst- und Tierwirtschaft und Gartenbau                             \\
  2                                          & Rohstoffgewinnung, Produktion und Fertigung                                \\
  3                                          & Bau, Architektur, Vermessung und Gebäudetechnik                            \\
  4                                          & Naturwissenschaft, Geografie und Informatik                                \\
  5                                          & Verkehr, Logistik, Schutz und Sicherhe                                     \\
  6                                          & Kaufmännische Dienstleistungen, Warenhandel, Vertrieb, Hotel und Tourismus \\
  7                                          & Unternehmensorganisation, Buchhaltung, Recht und Verwaltung                \\
  8                                          & Gesundheit, Soziales, Lehre und Erziehung                                  \\
  9 & Sprach-, Literatur-, Geistes-, Gesellschafts- und Wirtschaftswissenschaften, Medien, Kunst, Kultur und Gestaltung \\
  0                                          & Militär                                                                    \\ \hline
  \end{tabular}%
  }
  \caption{\label{tab: T3} Overview of classes Level 1 - Berufsbereiche (edited after \citep{Bundesagentur2011b})}
  \end{table}

\begin{table}[hb!]
  \center
  \resizebox{\textwidth}{!}{
  \begin{tabular}{llll}
  \hline
  \textbf{Name}      & \textbf{Level} & \textbf{Number of classes} & \textbf{Example}                               \\ \hline
  Berufsbereiche     & 1              & 10                         & 4: Naturwissenschaft, Geografie und Informatik \\
  Berufshauptgruppen & 2 & 37   & 43: Informatik-, Informations- und Kommunikationstechnologieberufe            \\
  Berufsgruppen      & 3              & 144                        & 434: Sofwareentwicklung                        \\
  Berufsuntergruppen & 4              & 700                        & 4341: Berufe in der Sofwareentwicklung         \\
  Berufsgattungen    & 5 & 1286 & 43412: Berufe in der Sofwareenetwicklung - fachlich ausgerichtete Tätigkeiten \\ \hline
  \end{tabular}%
  }
  \caption{\label{tab: T2} Overview of \ac{KldB} (edited after \citep{Bundesagentur2011b})}
  \end{table}


\begin{table}[]
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{clll}
  \hline
  \multicolumn{1}{l}{\textbf{Level of requirement}} & \textbf{Class ID} & \textbf{Name long} & \textbf{Name short} \\ \hline
  1 & xxxx1 & Helfer- und Anlerntätigkeit        & Helfer     \\
  2 & xxxx2 & fachlich ausgerichtete Tätigkeiten & Fachkraft  \\
  3 & xxxx3 & komplexe Spezialstentätigkeiten    & Spezialist \\
  4 & xxxx4 & hoch komplexe Tätigkeiten          & Experte    \\ \hline
  \end{tabular}%
  }
  \caption{\label{tab: T4} Overview of Level of requirements on Level 5} (edited after \citep{Bundesagentur2011b})
  \end{table}

With the \ac{KldB} 2010, a valuable and information-rich occupational classification was created for Germany that reflects the current trends in the labor market \citep{Paulus2013}. One strength relies upon the construction of the \ac{KldB}. Instead of just including expert knowledge into the Taxonomy, the development process is based on systematical consideration of occupations and statistical procedures for taxonomy development. Furthermore, the taxonomy was reviewed qualitatively several times concerning professions. 

Considering the expressiveness, the \ac{KldB} has some more benefits. Since the taxonomy is relatively recent, it reflects new job classes and market trends adequately. Further, the taxonomy provides a powerful tool to organize job titles into simple requirement classes by including the second dimension. In addition, the taxonomy also distinguishes between managerial, supervisory, and professional employees, which is also valuable information. Finally, the taxonomy also convinces with the possibility to switch to ``\ac{ISCO}" through its IDS and thus to normalize jobs to a global standard \citep{Bundesagentur2011b}. 
  
The \ac{KldB} dataset contains different information related to the structure described above. Some search words are given besides the class label, the level, and the title, on level 5 for each ``kldb". There are two types of keywords. First, job title search words that match the respective kldb. Second actual search words, with the help of which the associated kldb can be inferred. Therefore, these search words are beneficial knowledge for training classification algorithms because they contain ``kldb" specific words that are also often present in the job titles. The search words will be termed additional knowledge in the rest of the work. 
 
\subsection{Job title data}
Job titles can be scraped from the Federal Employment Agency's job board. Employers must provide additional data for each job posting, including the job title, as well as an internal documentation code that indicates a class in the \ac{KldB} taxonomy. There is an option to provide alternative documentation codes if more than one ``kldb" is believed. The data contains of job positings between June and November 2021. An example snippet of the scraped data is provided in the appendix \footnote{There are two versions of the raw data since the Federal Employment Agency changed during the scraping phase the data structure. Both versions are given in the appendix.} The documentation code is an internal number of the Federal Employment Agency, which can be uniquely assigned to a ``kldb", which, as already mentioned, is specified in the taxonomy data for each kldb. A snippet of the matched training data with the ``kldbs" is given in the appendix.

Initially, in total, the training data set contained 269788 examples. However, during the training phase, it became clear that there are problems, especially with the \ac{SVM} classifier, concerning the running time and memory. Due to limited resources, a sample with the same distribution from the long dataset had to be taken. A sample size of 15000 examples proved to be feasible. The data is divded into training and test data, with $0.25\%$ of the examples assigned to the test data. \footnote{Instead of splitting the training data once, the literature advises using cross-validation \citep{refaeilzadeh2009}. Cross-validation involves splitting the data and validating it in more than one round to ensure that performance is not random. For computational reasons, however, I do not apply this method. However, the code to perform cross-validation is included in the repository. It also outputs the confidence intervals for cross-validation.} Figures \ref{fig: F9} and \ref{fig: F10} show the distribution of the classes in level 1 and level 3 \footnote{The class distribution of the long dataset is given in the appendix.}. Both figures show the absolute number of examples for each ``kldb" in the respective levels. In figure \ref{fig: F10} the number of examples is colored with the belonging kldb level 1 class to give a better overview of the 144 ``kldbs" on level 3. In general, from both levels, it is clear that the data is not distributed equally. For the class distribution of level 1 on Figure \ref{fig: F9} class 1 and class 9 have really few examples, while class 2 has considerably more examples than all other classes. Although the data is imbalanced, all classes have at least some examples to get meaningful performance measurements. Note that ``kldb" 0  is missing. 0 stands for ``Militär". Jobs in this category are generally not posted frequently on the Employment Agency Job Board page, so it was not possible to get examples for this ``kldb". At level 3, the uneven distribution is even more apparent. Compared to level 1, the problem is that there are many classes with only one example, which is problematic for obtaining interpretable measurements. This problem will be discussed in the results part. Eight classes do not have any examples and thus cannot be trained with the classifier.   

\begin{figure}
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{training_data_short_L1.png}
    \caption{\label{fig: F9} Class distribution Level 1}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{training_data_short_L3.png}
    \caption{\label{fig: F10} Class distribution Level 3}
  \end{subfigure}
  \caption{Class distribution of the training data}
  \end{figure}


\section{Method}
\subsection{Overview of pipeline}

\begin{figure}[]
  \center
  \includegraphics[scale=0.5]{pipeline_MA.png}
  \caption{\label{fig: F8} Training Pipeline}
\end{figure}

The classification process is divided into several steps. Figure \ref{fig: F8} gives an overview of the procedure. In the first step, the \ac{KldB} and the job title data are focused on the necessary variables. This is done by matching the kldb ids from the \ac{KldB} data set with the internal documentation id from the job title data. In the following steps, the targeted data is preprocessed. This preprocessed data is then transformed into numerical vectors. For this purpose, different vector representations are created using five vectorization techniques. The transformed data is then reduced to lower dimensions using principal component analysis. The resulting data is the input for the classifier. In the last step, the four classifiers are trained. Note that the \ac{BERT} deep learning algorithm follows a different pipeline, which is why the algorithm is presented separately. Here, the data is directly input to the deep learning model after preprocessing, and the model is trained. The targeting of the data has already been described in the previous chapter. The procedure and the associated methods are explained in detail in the following.

\subsection{Preprocessing}
Having an important influence on the performance of a classifier \citep{uysal2014, hacohen2020, gonccalves2005}, preprocessing is a crucial step that has to be taken before vectorization. There are several standardized practices for preprocessing like tokenization, stemming, stop-word removal, and lowercasing \citep{alsmadi2019}. The methods of preprocessing are justified below.

The first preprocessing step is removing special characters, which is necessary because most titles contain slashes and brackets to distinguish gender forms in occupational titles. Some job titles contain emojis, such as asterisks, which need to be removed because they are not related to specific job titles. 

Capitalized words are usually converted to lowercase to treat them equally for the classification. However, lowercase can be at the same time harmful for the performance if the interpretation changes with converting. Considering the word `US", which refers to the United States of America. Converting to lowercase, it is equal to the pronoun ``us" , which can affect the performance if the differentiations play a role in the classification \citep{kowsari2019text}. Conversion makes sense for occupational names in German since capitalization plays a major role in German. In turn, I do not expect the conversion to cause problems often, as described in the example above, since occupational titles usually have specific names, and their lowercase variants do not often lead to other words or word classes. Thus in a second step, titles are all converted to lowercase. 

As in every text classification task, stop words removal is a common praxis. All stopwords are removed, which are listed in the German stopwords list of the Nature Language Toolkit package \citep{bird2009}. In addition, the data contained other peculiarities that justify other words removal besides stopwords. Job titles often contain words, such as ``employee" or ``effective immediately", that do not contain important information and are not specific to particular job titles. In order to identify such words, the frequencies of each word across all documents are calculated. These frequencies are then used to identify words, such as ``employee", that are common but not relevant. As a final preprocessing step, the identified words are deleted from all job titles.  

\subsection{Vectorization Techniques}
\subsubsection*{Count vectorizer}
The count vectorizer is one of the simplest methods for converting texts to vectors. It belongs to the family of \ac{BOW} models. \ac{BOW} models are based on vectors, where each dimension is a word from a vocabulary or corpus. The corpus is built by all words across all documents. \ac{BOW} models have two essential properties. First, they do not consider the order of the words, sequences, or grammar, which is why they are also called \ac{BOW}. Second, each word in the corpus is represented by its own dimension. Thus the vectors contain many zeros, especially for short texts, which is why they belong to the family of sparse vectors \citep{ajose2020}. Assuming that a corpus contains 1000 words, meaning each text is built only with these 1000 words, the vector for each text has a length of 1000, thus, producing sparse, high-dimensional vectors \citep{kulkarni2021, sarkar2016}

The values for the vector are generated by counting for each text the frequency of the words occurring. Considering a corpus including only the three words ``java", ``developer" and ``python", the titles ``java developer" and ``python developer" would be encoded as follows:


\begin{table}[hb!]
\center
  \begin{tabular}{llll}
  \hline
                   & java & developer & python \\ \hline
  java developer   & 1    & 1         & 0      \\
  python developer & 0    & 1         & 1      \\ \hline
  \end{tabular}
  \caption{\label{tab: T11} Encoding with count vectorizer}
  \end{table}


  The table \ref{tab: T11} results in the vectors $(1,1,0)$ and $(0,1,1)$. Note that if the title ``java developer" contains, for example, two times the word ``java", then the vector would change to (2,1,0). But since this is not a likely case for short text and especially job titles, the count vectorization here is for the most titles similar to one-hot vector encoding, which only considers the occurrence of the words, but not the frequency \citep{kulkarni2021, sarkar2016}

  While it is one of the most simple techniques, the count vectorizer has several limitations. The main downside is that it does not consider information like the semantic meaning of a text, the order, sequences, or the context. In other words, much information of the text is lost \citep{sarkar2016}. In addition, the count vectorizer does not consider the importance of words in terms of a higher weighting of rare words and a lower weighting of frequent words across all documents \citep{suleymanov2019}.

\subsubsection*{TFIDF vectorizer}
\ac{TF-IDF} belongs like the count vectorizer, to the family of \ac{BOW} and is as well a sparse vector. In contrast, to count vectorizer, it considers the importance of the words by using the \ac{IDF}. The main idea of \ac{TF-IDF} is to produce high values for words that often occur in documents but are rare over all documents. The \ac{TF} represents the frequency of a word t in a document d and is denoted by $tf(t,d)$. The \ac{DF}, denoted by $df$, quantifies the occurrence of a word over all documents. By taking the inverse of \ac{DF}, we get the \ac{IDF}. Intuitively the \ac{IDF} should quantify how distinguishable a term is. If a term is frequent over all documents, it does not help distinguish between documents. Thus the \ac{IDF} produces low values for common words and high values for rare terms and is calculated as follows \citep{sidorov2019, kuang2010}: 

\[idf(t) = log \frac{N}{df} \]

where N is the set of documents. The log is used to attenuate high frequencies. If a term occurs in every document, so $df = N$ the \ac{IDF} takes a value of 0 ($log (\frac{N}{N})$) and if a term is occuring only one time over all documents, thus distinguish perfectly the document from other documents, $idf(t) = log (\frac{N}{1}) = 1$. \citep{sidorov2019} Note that there are slight adjustments to calculate the \ac{IDF}. \citep{robertson2004} The implementation of sklearn package, which is used in this work uses an adapted calculation \citep{scikit-learn}:

\[idf(t) = log \frac{1+n}{1+df(t)} + 1 \] 

Given the $idf(t)$ and $tf$ the \ac{TF-IDF} can be obtained by multiplying both metrics. The implementation of sklearn normalize in addition the resulting \ac{TF-IDF} vectors $v$ by the Euclidean norm \citep{scikit-learn}: 

\[v_{norm} = \frac{v}{||v||_2} \]

Although \ac{TF-IDF} considers the importance of words, as a \ac{BOW} model, it suffers from the same limitation as count vectorizer of not taking semantic, grammatic, sequences, and context into account \citep{sarkar2016}. 

\subsubsection*{Word2Vec}
In contrast to the sparse techniques mentioned above, word embedding techniques are another popular approach for vectorization. Word embedding vectors are characterized by low dimensions, dense representation, and continuous space. They are usually trained with neural networks \citep{li2015, jin2016}. 

Word2vec, introduced by \citet{mikolov2013}, is one computationally efficient word embedding implementation. The main idea of word2vec is based on the distributional hypothesis, which states that similar words often appear in similar context \citep{sahlgren2008}. Thus, word2vec learns with the help of the context representations of words, which include the semantic meaning and the context. In such a way, similar words are encoded similarly \citep{sarkar2016}. 

There exist two variants of word2vec. The first variant is based on \ac{BOW}, the so-called \ac{CBOW}, which predicts a word based on surrounding words. In contrast, the second variant, Skip-Gram, predicts the context words from a word \citep{ajose2020, sarkar2016}. This study uses a pre-trained model from Google, which is trained with \ac{CBOW},. Thus in the following, the focus is on \ac{CBOW}. 

\ac{CBOW} word2vec is a 2-layer neural network with a hidden layer and an output softmax layer, which is visualized in figure \ref{fig: F1}. The goal is to predict a word, the so-called target word, by using the target word's context words. A certain window size defines the number of context words. If the window size is 2, the two words before and after the target word are considered. Given a vocabulary V, which is the unique set of the words from the corpus, each context word c is fed into the neural network, encoded with a one-hot encoding of the length of V, building vector $x_c$. Thus in figure \ref*{fig: F1} $x_{1k}$, for example, could be a one-hot encoded vector of the word before the target word. 

\begin{figure}[]
  \center
  \includegraphics[scale=0.5]{word2vecCBOW.png}
  \caption{\label{fig: F1} \ac{CBOW} \citep[6]{rong2014}}
\end{figure}

The weights between the input layer and the hidden layer are shown in figure \ref{fig: F1}. Taking the dimension of V and the hidden layer size N results in the $V \times N$ matrix W. Given that each row $v_w$ in W represents the weights of the associated word from the input and C equals to the number of context words, the hidden layer h is calculated as follows \citep{rong2014}:

\[ h = \frac{1}{C} W^T(x_1 + x_2 + ... +x_c) = \frac{1}{C}(v_{w_1} + v_{w_2} + ... + v_{w_C}) \]

Since the context words are encoded with one hot vector encoding, all except the respective word value in the vector, which is 1, will be 0. Thus calculating h is just copying the k-th row of matrix W, thus an n-dimensional vector, which explains the second part of the equation. The hidden-layer matrix builds later the word embedding, which is why the decision of the size of the hidden layer defines the dimensions later for the word embedding vector \citep{rong2014}

From the hidden to the output layer, a $N \times V$ weight matrix is used to calculate scores for each word in V. A softmax function is used to get the word representation. Calculating the error and using backpropagation, the weights are updated respectively, resulting in a trained neural network. Due to computational efficiency, word2vec is trained with hierarchical softmax or negative sampling instead of the softmax function. Both methods are efficient because they reduce the amount of weight matrix updates. \footnote{Since the focus is relying here on the word embeddings from the hidden layer and not the trained neural network itself, no further mathematical details will be given concerning the updating. For a detailed derivation see \citep{rong2014}} \citep{rong2014, simonton2017}.

Based on the given theoretical insights, I trained two word2vec models. Both models use a pre-trained model from Google and are fine-tuned with different data. The first model is fine-tuned with the complete dataset. The second model includes additional knowledge. The set-up is as follows: I use \ac{CBOW} Word2Vec models with a negative sampling technique. The hidden layer size and thus the word embedding vectors is 300 since the vectors have to have the same size as the pre-trained Google vectors. The minimal count of words is set to 1. The number of times the training data set is iterated, the epoch number is set to 10. Lastly, the window size for the context is set to 5. 

As the last step, the resulting word embeddings need to be processed in some way to get sentence embeddings for each job title. Word2vec cannot output sentence embeddings directly, which is why the word vector embeddings of each job title are averaged. 

\subsubsection*{Doc2vec}
Doc2vec, also known as paragraph vectors or Distributed Memory Model of Paragraph Vectors, is an extension method of word2vec, which outputs embeddings directly for each document \citep{lau2016}. It was proposed by \cite{le2014}. Doc2vec can be used for a variable length of paragraphs. Thus, it is applicable for more extensive documents, but also for short sentences like job titles \citep{le2014}. 

The main idea is, like for word2vec, to predict words in a paragraph. To do so, a paragraph vector and word vectors, like in word2vec, for that paragraph are concatenated. The paragraph vector ``acts as memory that remembers what is missing from the current contest - or the topic of the paragraph" \citep[3]{le2014}. Thereby the paragraph vectors are trained with stochastic gradient descent and backpropagation. Similar to word2vec practical implementation, use hierarchical softmax or negative sampling to fast up training time \citep{lau2016}. 

\begin{figure}[]
  \center
  \includegraphics[scale=0.5]{doc2vec.png}
  \caption{\label{fig: F2} Doc2vec - Distributed Memory Model of Paragraph Vectors \citep[3]{le2014}}
\end{figure}

In figure \ref{fig: F2} the algorithm is visualized. The neural networks take word vectors and a paragraph vector as input. While the word vectors are shared over all paragraphs, each paragraph's paragraph vectors are unique. The paragraphs are represented as unique vectors in a column of a matrix D. The word vectors are expressed in the matrix W as before. In order to predict the word, both vectors are combined, for example, by averaging or concatenating. The doc2vec implementation described by \citet{le2014} and also used in this work here concatenate the vectors. Formally this only changes the calculation of h. \citep{lau2016}

Since doc2vec is a word embedding method, it has the same advantages mentioned for word2vec. In addition, doc2vec takes the word order into account. At least in the same way of a large n-gram \citep{le2014}. Besides the model explained above, doc2vec also comes in a second variant, the so-called Distributed Bag of Words of Paragraph model, which ignores the word order. It is not clear which model performs better, although the inventor of doc2vec propose the first version \citep{lau2016}

Based on this discussion, I created two Distributed Memory Models of Paragraph Vectors. Instead of fine-tuning a pre-trained model, I trained two custom models, one with the training data and one including the additional knowledge. I set the vector size to 300 with a window size of 5, a minimal count of 1, and trained ten epochs. Like word2vec, the models are trained with negative sampling. 

\subsubsection*{BERT}
\ac{BERT} uses a multi-layer bidirectional Transformer encoder as the architecture. This transformer architecture was introduced by \citet{vaswani2017}. It consists of an encoder and a decoder and makes use of self-attention. Both encoder and decoder stack includes some identical layer. Each of them includes two sub-layers: A multi-head attention and a feedforward network layer \footnote{A simplified visualization with a two-layer encoder, as well the architecture of a  can be found in the Appendix.}. It is out of the scope to elaborate on the technical details of the attention mechanism, which is why a simplified explanation is given. 

The self-attention mechanism improves the representation of a word, represented by a matrix X by relating it to all other words in the sentence. In the sentence ``A dog ate the food because it was hungry" \citep[10]{ravichandiran2021} the self-attention mechanism, for example, could identify by relating the word to all other words, that ``it" belongs to ``dog" and not to ``food". 
In order to compute the self-attention of a word, three additional matrices, the query matrix Q, the key matrix K, and a value matrix V are introduced. Those matrices are created by introducing weights for each of them and multiplying those weights with X. Based on those matrices, the dot product between the Q and the K matrix, a normalization and a softmax function is applied in order to calculate an attention matrix Z  \footnote{For a step-by-step calculating see \citep{ravichandiran2021}}. \ac{BERT} uses a multi-head-attention, which means that multiple Z attention matrices instead of a single one are used.

\ac{BERT} takes one sentence or a pair of sentences as input. To do so, it uses a WordPiece tokenizer and three special embedding layers, the token, the segment, and the position embedding layer for each token, which is visualized in \ref{fig: F4}. A WordPiece tokenizes each word that exits in the vocabulary. If a word does not exist, words are split as long a subword matches the vocabulary or an individual character is reached. Hashes indicate subwords. For the token embeddings, the sentence is tokenized first, then a [CLS] is added at the beginning of the sentence and [SEP] token at the end. The job titles ``java developer" and ``python developer", for example, become tokens as shown in the second row of \ref{fig: F4}. In order to distinguish between the two titles, a segment embedding is added, indicating the sentences. Last the \ac{BERT} model takes a position embedding layer, which indicates the order of the words. For each token, those layers are summed up to get the representation for each token \citep{devlin2018,ravichandiran2021}.

\begin{figure}[hb!]
  \center
  \includegraphics[scale=0.5]{BERTInput_own.png}
  \caption{\label{fig: F4} Input \ac{BERT} \citep[5]{devlin2018}}
\end{figure}

\begin{figure}[hb!]
  \center
  \includegraphics[scale=0.5]{BERTOverview.png}
  \caption{\label{fig: F3} Overview \ac{BERT} \citep[3]{devlin2018}}
\end{figure}


The \ac{BERT} algorithm can be described in two steps. First, the pretraining phase, which is illustrated on the left-hand side of the figure \ref{fig: F3} and the fine-tuning phase, which is visualized on the right-hand side. The pretraining phase consists of two jobs: Masked language modeling and next sentence prediction. 

\subsubsection*{Pretraining}
Masked language modeling means that a percentage of the input tokens are masked at random. For example, the job title `"python developer" could be masked as follows: [[CLS] python [MASK] [SEP]]. Since in fine-tuning tokens are not masked, a mismatch would occur between fine-tuning and pretraining, which is why not all of the masked tokens are matched with a [mask] token, but also with a random token or the real tokens \footnote{There are specific rules of how to mask. See \citet{devlin2018} for detailed implementation}. Instead of predicting the complete sentence, \ac{BERT} trains to predict the masked tokens. The prediction is performed with a feed-forward network and a softmax activation \citep{devlin2018,ravichandiran2021}. 

The second task retakes two sentences but predicts whether the second sentence follows the first one, which helps understand the relationship between the sentences. Each sentence pair is labeled with either isNext or NotNext. By using the [CLS] token, which has the aggregating representation of all tokens, a classification task of whether a sentence pair is isNext or NotNext can be carried out \citep{ravichandiran2021,devlin2018}

The pretraining of \ac{BERT} is in contrast to the fine-tuning process computationally expensive. There are plenty of pre-trained \ac{BERT} models for the German case, like ``bert-base-german-cased" or ``distilbert-base-german-cased" \footnote{All german \ac{BERT} models are open source and are accessible through the transformers library \citep{wolf2020}}. In an evaluation of German pre-trained language models, \citep{assenmacher2021} conclude that the ``bert-base-german-dbmd-uncased" algorithm works quite well. Following their results and own tests on different models, "bert-base-german-dbmd-uncased`` and ``bert-base-german-cased" seem to have the best results, which is why they are used for the fine-tuning process. Both models consist of 12 encoder layers, 12 attention heads, and 768 hidden units, resulting in 110 million parameters. The first model was trained with 16GB of German texts, the second model with 12GB of German texts. 

\subsubsection*{Fine-Tuning}
The second phase, fine-tuning, can be performed differently depending on the task. Either the weights of the pre-trained model are updated during the classification process, or the pre-trained model is first fine-tuned and then used as a feature extractor. 

I train two models with \ac{BERT}. While the first model includes a classification layer, in the following named as \ac{BERT} deep learning model, the second model applies \ac{BERT} as a feature extraction method, in the following named \ac{BERT} vectorizer. 

The \ac{BERT} deep learning model is fine-tuned with the complete training data set. Practically this is done by converting the sentences of the dataset to the appropriate data format as described above and training it with the supervised dataset on some epochs, which then outputs the labels. From a theoretical point of view, the last hidden state of the [CLS] token with the aggregation of the whole sentence is used for the classification. In order to get the labels \ac{BERT} uses a softmax function \footnote{The explanation of the softmax function follows in the chapter of the classifiers} \citep{sun2019}. As already stated in the literature, it is not well-understood what exactly happens during the fine-tuning. An analysis of \citet{merchant2020} for other NLP tasks than text classification indicates that the fine-tuning process is relatively conservative in the sense that they affect only a few layers and are specific for examples of a domain. The training set-up is as follows: Testing different epoch numbers indicates that smaller epoch size has better results for the model, which is why I fine-tune in 6 epochs. For the optimization, an adam algorithm, a gradient-based optimizer \citep{kingma2014}, with a learning rate of $1e^{-5}$ is used. 

In order to get sentence embeddings, different strategies, like averaging the output layer of \ac{BERT} or using the [CLS] token, is applied. Another method, developed by \citet{reimers2019} is sentence-\ac{BERT}, which is computationally efficient and practicable to implement. Since it facilitates encoding sentences directly into embeddings, I use it for the \ac{BERT} vectorizer. The model is constructed with the "bert-base-german-dbmd-uncased`` model and a pooling layer. The pooling layer is added to output the sentence embeddings. The fine-tuning process uses a siamese network architecture to update the weights. Figure \ref{fig: F5} shows the architecture. The network takes pairs of sentences with a score as an input. Those scores indicate the similarity between the sentences. The network updates the weights by passing the sentences through the network, calculating the cosine similarity, and comparing to the similarity score \citep{reimers2019}.
I create from the job title dataset and the kldb dataset pairs of similar and dissimilar job titles or search words. Similar pairs are pairs from the same ``kldb" class. As a score, I choose 0.8. Dissimilar pairs are defined as pairs that are not from the same class. The score is 0.2. Building all combinations of titles and the search words for each class would result in a huge dataset. For example, class 1 of the training data set has 2755 job titles, which results in ${2755 \choose 2} = 3793635$ pairs for class 1. Since this is computationally too expensive, I randomly choose pairs of job titles. For level 1, the following samples are drawn: From the job title data for each class, I used 3250 pairs. The same is also for the searchwords for each class. For unsimilar pairs, I used 1750 job title pairs, not from the same class. Same for search words. This results in total number of $(3250+3250) \times 9 + 2 \times 1750 = 62000$ pairs for the finetuning of level 1. \footnote{The determination of the exact number of pairs is exploratory, and run time and performance were taken into account}. For level 3, the procedure is the same, only the numbers differ. I used for similar pairs for job titles and searchwords 400 for each class and for the unsimilar 6000 pairs, which gives a total number of $(400 + 400) \times (136-7) + 2 \times 1500 =  61200$. Classes with only one example are not considered because building pairs is impossible. 

\begin{figure}[hb!]
  \center
  \includegraphics[scale=0.5]{SBERT.png}
  \caption{\label{fig: F5} Sentence-\ac{BERT} siamese architecture \citep[3]{reimers2019}}
\end{figure}

\subsection{Dimensionality reduction}
Dimensionality reduction techniques like \ac{PCA} play an essential role in reducing computation time and saving computing resources \citep{ayesha2020}. Figure \ref{fig: F6} shows the running time of all three classifiers with different data sizes and with and without \ac{PCA} dimensionality reduction. 


\begin{figure}[hb!]
  \center
  \includegraphics[scale=0.5]{running_time_PCA.jpeg}
  \caption{\label{fig: F6} Running time of word2vec with different data sizes}
\end{figure}

The input of the classifiers is the word2vec word embeddings without the additional information. The bright lines show the running times without dimensionality reduction, while the dark-colored lines report the running time with \ac{PCA} transformation. It becomes clear that the runtime for the transformed embeddings is generally lower. The magnitude of the differences is almost irrelevant for a data set of 500. However, for the non-transformed embeddings, the runtime increases considerably with the data set size for all classifiers. This is most evident with \ac{RF}. The runtime of the transformed embeddings also increases for all classifiers. However, it does so at a much slower pace. Therefore, it can be concluded that the transformation contributes to keeping the runtime lower for large data sets. As already described in chapter 3, the training data set is pretty large, so it is reasonable to reduce the dimensions. 

\ac{PCA}, one of the most popular techniques for dimensionality reduction, aims to reduce a high-dimensional feature space to a lower subspace while capturing the essential information \citep{tipping1999, bisong2019}. The main idea is to use linear combinations of the original dimensions, so-called principal components, to reduce the dimensional space \citep{bro2014,geladi2017}.

Conceptually, the covariance matrix for the word embeddings is obtained in the first step. The covariance matrix, denoted by $\textbf{X}$, captures the linear relationships between the features of the word embeddings. In the next step, the eigenvectors of $\textbf{X}$ are calculated. The eigenvector of X is defined as \citep{bro2014}: 

\[ \textbf{X}z = \lambda z \]

where $z$ is the eigenvector and $\lambda$ the eigenvalue. In order to decompose $\textbf{X}$ to get the eigenvalue, Singular Value Decomposition is applied. The eigenvalues are sorted from highest to lowest, and the most significant components $n$ are kept. In order to choose $n$, the variance explained by the principal components is considered. The explained variance is set to $0.95$. In order to transform the data, a feature vector is generated. This vector contains the most $n$ significant eigenvalues. After transposing the mean-adjusted word embedding and the feature vector, the embeddings can be transformed by multiplying both transposed vectors \citep{smith2002}. 

\subsection{Classifier}
In the following, based on a theoretical discussion of each classifier, the exact modeling is justified. The focus and depth of the explanations of the properties of the classifiers, such as decision function or regularization, depends on the need to explain the settings of the classifiers. Therefore, not all properties are described to the same extent for all classifiers. The \ac{BERT} deep learning model is already described in chapter 4.3. 

\subsubsection{Logistic Regression}
\ac{MLR}, a generalized linear model, is one of the most used analytical tools in social and natural science for exploring the relationships between features and categorical outcomes. For solving classification problems, it learns weights and a bias(intercept) from the input vector. Figure \ref{fig: F7} illustrate the idea of the calculation of \ac{MLR}. To classify examples, first, the weighted sum of the input vector is calculated. For multi-classification, the weighted sum has to be calculated for each class. Thus given a $f \times 1$ feature vector x with $[x_1, x_2, ..., x_f]$, a weight vector $w_k$ with $k$ indicating the class k of set of classes K, a bias vector $b_k$ the weighted sum the dot product of $w_k$ and $\textbf{x}$ plus the $b_k$ defines the weighted sum. Representing the weight vectors of each class in a $[K \times f]$ matrix $\textbf{W}$, formally the weighted sum is $\textbf{W}x+b$. In Figure \ref{fig: F7} the blue lines, for example, are a row in $\textbf{W}$ and are the weight vectors related to a class labeled with 1 \citep{jurafsky2021}.

\begin{figure}[hb!]
  \center
  \includegraphics[scale=0.5]{LR.png}
  \caption{\label{fig: F7} Multinomial Logistic Regression (edit after \citep[p.]{jurafsky2021})}
\end{figure}

In a second step, the weighted sums are mapped to a value range of $[0,1]$ to classify the input. While binary logistic regression uses a sigmoid function to do so, \ac{MLR} needs a generalized sigmoid function. This generalization is called the softmax function, which outputs probabilities for each of the classes, which is why \ac{MLR} is also often called softmax regression in the literature. These probabilities models for each class $p(y_k = 1|x)$.

Similar to sigmoid function, but for multiple values the softmax function maps each value of an input vector z with $[z_1, z_2, ..., z_K]$ to a value of the range of $[0,1]$. Thus outputting a vector of length z. All values together summing up to 1. Formally it is defined as: 
\[ softmax(z_i) = \frac{exp(z_i)}{\sum^K_{j=1} exp(z_j)} \text{ } 1 \leq i \leq K \]
Then the output vector y can be calculated by 

\[ \hat{y} = softmax(\textbf{W}x+b) \]

The goal of weight and bias training is to `"maximize the log probability of the true y labels" of the input data. This is commonly done by minimizing a generalized cross-entropy-loss function for \ac{MLR}. Different methods exist for solving the optimization problem, like stochastic gradient descent or limited-memory Broyden-Fletcher-Goldfarb Shannon solver. The latter converges rapidly and is characterized by a moderate memory, which is why it can converge faster for high-dimensional data \citep{fei2014, scikit-learn}. 

For \ac{MLR} it is common to add regularization parameter. This avoids overfitting and ensures that the model is more generalizable to unseen data. The idea is to penalize weights with a good classification but use a lot of high weights, more than weights with good classification but smaller weights. There are two popular penalty terms, the L1 and the L2 penalty. While for L1, the absolute values of the weights are summed and used as the penality term, L2 regularizes with a quadratic function of the weights. With the regularization, a parameter C is introduced to control the strength of the regularization. C is a positive value, and if smaller, it regularizes stronger \citep{jurafsky2021}. 

The following setting will be used for the \ac{MLR}: A \ac{MLR} with a L2 penalty with a C value of 1 is used. Since the input vectors, especially for count vectorizer and for \ac{TF-IDF} are high-dimensional, the limited-memory Broyden-Fletcher-Goldfarb-Shannon solver is set for solving. For some training, converge problems appeared, which is why the maximal iteration of the classifier is set to 10000.

\subsubsection{Support Vector Machines}
 The general idea of a \ac{SVM} is to map ``the input vectors x into a high-dimensional feature space Z through some nonlinear mapping chosen a priori [...], where an optimal separating hyperplane is constructed'' \citep[138]{Vapnik2000}. In \ac{SVM} this optimal hyperplane maximizes the margin, which is the distance from the hyperplane to the closest points, so-called Support Vectors, across both classes \citep{Han2012}. Formally, given a training data set with n training vectors $x_i \in R^n, i = 1,....,n$ and the target classes $y_1,...y_i$ with $y_i \in \{-1, 1\}$, the following quadratic programming problem (primal) has to be solved in order to find the optimal hyperplane:
\[\min_{w,b} \frac{1}{2}w^{T}w \] 
\[\text{subject to } y_i(w^T\phi(x_i)+b) \geq 1\]

where $\phi(x_i)$ transforms $x_i$ into a higher dimensional space, $w$ corresponds to the weight and $b$ is the bias \citep{Chang2001,Jordan2006}
The given optimization function assumes that the data can be separated without errors. This is not always possible, which is why \cite{Cortes1995} introduces a soft margin \ac{SVM}, which allows for misclassification \citep{Vapnik2000}.
By adding a regularization parameter $C$ with $C > 0$ and the corresponding slack-variable $\xi$ the optimization problem changes to \citep{Chang2001, Han2012}: 
\[\min_{w,b} \frac{1}{2}w^{T}w + C \sum_{i=1}^n \xi_i \] 
\[\text{subject to } y_i(w^T\phi(x_i)+b) \geq 1 - \xi_i, \] 
\[\xi_i \geq, i = 1,...,n\]

Introducing Lagrange multipliers $\alpha_i$ and converting the above optimization problem into a dual problem the optimal $w$ meets \citep{Chang2001, Jordan2006}:
\[w = \sum_{I=1}^n y_i\alpha_i\phi(x_i)\]

with the decision function \citep{Chang2001}:
\[\text{sgn } (w^T\phi(x)+b) = sgn(\sum_{i=1}^n y_i \alpha K(x_i, x) +b)\]

$K(x_i, x)$ corresponds to a Kernel function, which allows calculating the dot product in the original input space without knowing the exact mapping into the higher space \citep{Han2012, Jordan2006}. 

In order to apply \ac{SVM} to multiclass problems, several approaches have been proposed. One strategy is to divide the multi-classification problem into several binary problems. A common approach here is the one-against-all method. In this method, as many \ac{SVM} classifiers are constructed as there are classes k. The k-th classifier assumes that the examples with the k label are positive labels, while all the other examples are treated as negative. Another popular approach is the one-against-one method. In this approach $k(k-1)/2$ classifiers are constructed, allowing to train in each classifier the data of two classes \citep{Hsu2002}. Besides dividing the multiclass problem into several binary problems, some researchers propose approaches to solve the task in one single optimization problem, like \citet{Crammer2001}. \footnote{For a detailed overview of all different methods and the method of \citet{Crammer2001} see \citet{Hsu2002,Crammer2001}}. 

In order to find a robust classifier, I checked \ac{SVM}'s with different parameters for the \ac{SVM}, as well as different multiclass approaches. It appears that a \ac{SVM} using a soft margin with a $C=1$ and a one-vs-rest approach has the best results. I also test different kernels, like  RBF Kernel or linear kernel. The linear kernel, formally  $k(x, x') = x^Tx'$, achieved the best results, which is why I chose it for the classifier. 

\subsubsection{Random Forest Classifier}
In contrast to the previous two classifiers, \ac{RF} is an ensemble learning technique. The main idea of ensemble learning techniques is to create several learners and combine them. Those learners are, for example, a decision tree or neural networks and are usually homogeneous, meaning that each learner is based on the same machine learning algorithm. The different ensemble techniques are built on three pillars: the data sampling technique, the strategy of the training, and the combination method \citep{polikar2012, zhou2009}. 

The first pillar, data sampling, is important because it is not desirable to have the same outputs for all classifiers. Thus ensemble techniques need diversity in the output, which means the outputs should be optimally independent and negatively correlated. There are well-established methods for achieving diversity. For example bagging techniques fall back to bootstrap \citep{polikar2012}. The second pillar raises the question of which techniques should be applied to train the learners of the method. The most popular strategies for the training are bagging and boosting \citep{polikar2012}. The last pillar is about the combining method. Each classifier of the method will output an individual classification result, and those results have to be combined in some way to achieve an overall result. There are plenty of methods like majority voting or borda count \citep{polikar2012}.

\ac{RF} uses as individual classifier decision trees. Before discussing \ac{RF} in more detail within the three pillars described above, a brief discussion of the decision tree is given in order to understand the mechanism and training procedure of the classifiers. 

The main idea of the decision tree algorithm is to `"break up a complex decision into a union of several simple decisions" \citep[660]{safavian1991} by using trees, with a root node on the top, intermediate nodes, and leaf nodes on the bottom. All possible splittings are checked and split according to the best feature for the root node and each intermediate node. Each leaf node leads to one of the classification labels. Examples are then classified by traversing the tree from top to bottom and choosing the branch that satisfies the attribute value for the example at each intermediate node. The construction of a Decision tree is a recursive procedure \citep{Berthold2020, xia2008, cutler2012}. The algorithm stops for a specific node if all training set examples belong to the same class or if there are no features left for splitting. This might end in a tree with a high depth, which is why pruning is often applied to avoid overfitting of the tree \citep{Berthold2020}.

There are two important points to discuss in constructing. First, the types of splitting and second splitting criterion. There are mainly three types of splits: Boolean splits, nominal splits, and continuous splits. The latter chooses a particular value from the continuous feature as the splitting value \citep{cutler2012, Berthold2020}. For example, considering a word embedding x with 300 dimensions and a node t of a decision tree, which is split into the nodes $t_{left}$ and $t_{right}$. The node t could have the split $x[209] <= 0.336$. Examples with a value smaller than or equal $0.336$ at the dimension index $209$ of the embedding vector follow the branch to $t_{left}$, while all other examples follow the branch to $t_{right}$.

The splitting criterion is essential to identify the best feature for splitting. Intuitively, the criteria should split the data in such a way that leaf nodes are created fast \citep{Berthold2020}. Several measurements obtain the best split for each node, like Gini impurity or information gain. Since \ac{RF} uses Gini impurity, only this criterion will be discussed in detail. 

The gini value indicates the purity of a dataset D with n classes. It is defined as follows \citep[3156]{yuan2021}: 

\[Gini(D) = 1 - \sum_{i=1}^n p^2_i\]

$p_i$ is the probability that a class n occurs in D. The more pure D is, the lower the value of the Gini value. In order to determine the best feature k, the dataset is partitioned based on feature k. For continuous features, as in word embeddings, this is done by a continuous split. Defining V as the total number of subsets and $D^v$ as one of the subsets, the Gini impurity for a feature k can be calculated as follows \cite{yuan2021}: 

\[\text{Gini index}(D,k) = \sum_{v=1}^V \frac{|D^v|}{|D|} Gini(D^v) \]

Conceptually the Gini index is the weighted average of the Gini value for each subset of D based on a feature k. Thus subsets with more samples are weighted more in the Gini index. The optimal feature $k^*$ is then determined by minimizing the Gini impurity overall features K \citep[3156]{yuan2021}: 

\[ k^* = arg \min_{k \in K} \text{Gini index} (D,k) \]. 

Based on the above theoretical explanations of the foundations of the decision tree, researchers have developed several algorithms to train decision trees, like Iterative Dichotomiser 3, C4.5. or \ac{CART}. \ac{CART} produces depending on the target variable classification (for categorical variables) or regression trees (for numerical variables). It constructs only binary trees. Thus each split is into two nodes. The algorithm uses as impurity measurement Gini index, and it can handle numerical and categorical input \citep{brijain2014}.

\ac{RF} belongs to the family of bagging ensemble techniques.
Bagging selects a single algorithm and trains several independent classifiers. The sampling technique is sampling with replacement (bootstrapping). Bagging combines the individual models by using either majority voting or averaging. \ac{RF} differentiates from the classic bagging method in the way that it also allows choosing a subset of features for each classifier from which the classifiers can select instead of allowing them to select from the complete range of features \citep{polikar2012, zhou2009, Berthold2020}. 

Formally \citet{breiman2001}, who introduced mainly the \ac{RF} algorithm, defines the classifier as follows:

\begin{quote}
`"A random forest is a classifier consisting of a collection of tree-structured classifiers $\{h(\textbf{x}, \Theta_k), k = 1, ...\}$ where the ${\Theta_k}$ are independent identically distributed random vectors and each tree casts a unit vote for the most popular class at the input $\textbf{x}$." \citep[6]{breiman2001}. 
\end{quote}
$\Theta_k$ is a random vector created for each k-th tree. $\Theta_k$ must be independent of the vectors $\Theta_1...\Theta_{k-1}$, thus from all random vectors of the previous classifiers. Although the distribution of the random vectors remains. Combined with the training set, with $\textbf{x}$ as the input vector a classifier $h(\textbf{x}, \Theta_k)$ is constructed \citep{breiman2001}. The random component $\Theta_k$ is not explicitly used in practical implementation. Instead, it is rather used implicitly to generate two random strategies \citep{cutler2012}. The first strategy is bootstrapping. Thus drawing sample with replacement from the training data set. In order to estimating generalization error, correlation and variable importance \citet{breiman2001} applied out-of-bag estimation. Out-of-bag estimation leaves out some portion of the training data in each bootstrap. The second strategy is to choose a random feature for the splitting. Thus at each node from the set of features, only a subset is used to split. \ac{RF} does not use pruning. The trees grow by applying \ac{CART}-algorithm. \ac{RF} uses as combination method for classification unweighted voting \citep{cutler2012}.

Based on the above explanations, the implemented \ac{RF} has the following setting: The number of learners is 100. Gini is used as the splitting criterion. The maximal number of features is $\sqrt{\text{number of features}}$. Note that sklearn, which is used to implement \ac{RF} here, uses an optimized algorithm of \ac{CART}. \citep{scikit-learn}.

\section{Result}
\subsection{Evaluation metrics}
Appropriate measurements are crucial for obtaining a qualitative comparison in the performance between the algorithms. There exist several metrics for the evaluation of classification approaches in the literature \citep{Fatourechi2008}. In the following, the selected measurements for performance measurement will be justified. 

Most metrics rely on a confusion matrix. For the multiclass case, this confusion matrix is defined as follows \citep{Kautz2017}: 
\begin{table}[hb!]
  \center
  \begin{tabular}{lllll}
  \hline
            & positive examples      &           &             &             \\ \hline
  positive prediction  & $c_{1,1}$ & $c_{1,2}$ & $\dots$     & $c_{1,n}$   \\
            & $c_{2,1}$ & $c_{i,j}$ &             &             \\
            &  $\vdots$         &           & $\ddots$ &   $\vdots$\\
            & $c_{n,1}$ &           & $\dots$     & $c_{n,n}$   \\ \hline
  \end{tabular}
  \caption{\label{tab: T1} Confusion Matrix (edited after \citep[113]{Kautz2017}}
  \end{table}

From the confusion matrix follows that $c_{i, j}$ defines examples which belong to class j and are predicted as class i. Based on the confusion matrix the true positives of a current class $m$ can be defined $tp_{m} = c_{m, m}$, thus examples which are correctly predicted as the current class m. The false negatives are defined as those examples which not belonging to the current class m, but are predicted as k. Formally $fn_{m} = \sum_{i=1, i \neq m}^n c_{i, m}$. Next, the true negatives are examples belonging to the current class m, but are not predicted as m. Formally $tn_{m} = \sum_{i=1, i\neq m}^n \sum_{j=1, j \neq m}^n c_{i,j}$. Last, false positives are defined as examples not belonging to class m, but are predicted as such. Formally this can be expressed as: $fp_{m} =  \sum_{i=1, i \neq m}^n c_{m, i}$ \citep{Kautz2017}

The \ac{OA} is one of the most common metrics for performance evaluation. It represents how well the classifier classifies across all classes correctly. Given that N is the number of examples, formally, the \ac{OA} can be expressed as: 
\[OA = \frac{\sum_{i=1}^m tp_i}{N} \]

An accuracy of 1 means that all examples are correctly classified. 0 means that each example is classified with the wrong class. \citep{Berthold2020}
Although \ac{OA} is a widely used metric, it is criticized for favoring the majority classes, thus not reflecting minority classes appropriately in unbalanced datasets \citep{Berthold2020, Fatourechi2008}

Two more popular metrics are precision and recall. Precision represents how well the classifier detects actual positive examples among the positive predicted examples. Recall, also called sensitivity, in contrast, represents how many examples are labeled as positive among the actual positive examples \citep{Berthold2020}. For the multiclass scenario, two different calculation approaches for each of the metrics are proposed: micro and macro average \citep{Branco2017}. In the macro approach, first, the metric is calculated for each class m against all other classes. Then, the average of all of them is built. Formally, given that $K$ is the total number of classes: 

\[precision_{macro} = \frac{1}{M} \sum_{i=1}^m \frac{tp_{i}}{tp_{i} + fp_{i}}\]
\[recall_{macro} = \frac{1}{M} \sum_{i=1}^m \frac{tp_{i}}{tp_{i} + fn_{i}}\]

In contrast the micro approach aggregates the values, which can be formally expressed as follows: 

\[precision_{micro} = \frac{\sum_{i=1}^m tp_i}{\sum_{i=1}^m tp_i + fp_i}\]
\[recall_{micro} = \frac{\sum_{i=1}^m tp_i}{\sum_{i=1}^m tp_i + fn_i}\]

There is a trade-off between precision and recall \citep{Buckland1994}. The F-measure capture both precision and recall by taking the harmonic mean between both. It is calculated as follows \citep{Branco2017,Pan2016}:  

\[F_{micro} = 2 \cdot \frac{precision_{micro} \cdot recall_{micro} }{precision_{micro} + recall_{micro} }\ \]

\[F_{macro} = 2 \cdot \frac{precision_{macro} \cdot recall_{macro} }{precision_{macro} + recall_{macro} }\ \]

A closer look at the formula of the micro scores shows that actually, the micro-precision score and the recall score are the same since aggregating the false negatives and aggregating the false positive results in the same number. If the precision and recall are the same, it follows from the F-measure calculation that it has to be as well equal. And even further, the micro score is reducing to the accuracy, thus suffering from the same problem as accuracy \citep{grandini2020}

Since the job title classification involves multiclass classification and the descriptive analysis shows that the data is unbalanced, it is not reasonable to base the evaluation solely on the \ac{OA}. Showing that micro scores of precision, recall, and f1 reduce to accuracy, it is crucial to take the macro scores into account to capture the performance of the minority classes well.

\subsection{Experimental results}
As explained at the top of this work, the results are compared from three perspectives: Vectorization techniques, classification algorithms, and enrichment with additional knowledge. Table \ref{tab: T5} and \ref{tab: T6} show the results of level 1 for all vectorization methods. Each row denotes a vectorization technique. Each column represents a classifier. Table \ref{tab: T5} reports the accuracy while table \ref{tab: T6}  report the macro precision(p), recall(r) and f1(F1) scores. The word2vec and doc2vec without additional knowledge training are marked with $I$, the ones with additional knowledge by $II$. The results of the \ac{BERT} deep learning model are reported separately in table \ref{tab: T12}. 


\begin{table}[hb!]
  \center
\begin{tabular}{lrrr} 
  \hline
  {} &    \textbf{LR} &   \textbf{SVM} &    \textbf{RF} \\
  \hline
CountVectorizer &  0.72 &  0.69 &  0.65 \\
TFIDF           &  0.72 &  0.69 &  0.65 \\
Word2Vec\_I      &  0.54 &  0.53 &  0.61 \\
Word2Vec\_II     &  0.54 &  0.52 &  0.62 \\
Doc2Vec\_I       &  0.48 &  0.46 &  0.56 \\
Doc2Vec\_II      &  0.45 &  0.42 &  0.53 \\
BERT            &  0.78 &  0.78 &  0.77 \\
  \hline
  \end{tabular}
  \caption{\label{tab: T5} Evaluation of Level 1 classification - Accuracy}
\end{table}


\begin{table}[]
    \resizebox{\textwidth}{!}{%
  \begin{tabular}{llll}
  \hline
  {} &                          \textbf{LR} &                         \textbf{SVM} &                          \textbf{RF} \\
  \hline
  CountVectorizer &  p: 0.76, r: 0.61, F1: 0.66 &  p: 0.72, r: 0.58, F1: 0.63 &  p: 0.66, r: 0.53, F1: 0.57 \\
  TFIDF           &  p: 0.77, r: 0.61, F1: 0.65 &  p: 0.73, r: 0.58, F1: 0.62 &  p: 0.67, r: 0.53, F1: 0.57 \\
  Word2Vec\_I      &  p: 0.58, r: 0.39, F1: 0.42 &   p: 0.46, r: 0.40, F1: 0.41 &  p: 0.58, r: 0.52, F1: 0.54 \\
  Word2vec\_II     &  p: 0.59, r: 0.41, F1: 0.45 &  p: 0.48, r: 0.41, F1: 0.43 &  p: 0.59, r: 0.53, F1: 0.55 \\
  Doc2Vec\_I       &  p: 0.51, r: 0.33, F1: 0.35 &  p: 0.41, r: 0.33, F1: 0.34 &   p: 0.59, r: 0.40, F1: 0.43 \\
  Doc2Vec\_II      &   p: 0.54, r: 0.30, F1: 0.32 &   p: 0.37, r: 0.30, F1: 0.31 &   p: 0.55, r: 0.38, F1: 0.40 \\
  BERT            &  p: 0.76, r: 0.76, F1: 0.76 &  p: 0.75, r: 0.77, F1: 0.76 &  p: 0.78, r: 0.72, F1: 0.74 \\
  \hline
  \end{tabular}
    }
  \caption{\label{tab: T6} Evaluation of Level 1 classification - macro}
  \end{table}


\begin{table}[]
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{llllllll}
  \hline
   &
    \textbf{accuracy} &
    \textbf{precision macro} &
    \textbf{recall macro} &
    \textbf{f1 macro} & \\ \hline
  \textbf{BERT clf level 1} &
    0.76 &
    0.71 &
    0.71 &
    0.71 & \\
  \textbf{BERT clf level 3} &
    0.44 &
    0.20 &
    0.18 &
    0.17 &\\ \hline
  \end{tabular}
  }
  \caption{\label{tab: T12} Evaluation of Level 1 and 3 \ac{BERT} deep learning model}
  \end{table}

Comparing the accuracy of the vectorization techniques the \ac{BERT} vectorizer outperforms with approx $78\%$ accuracy the other methods. A look at the macro table \ref{tab: T6} for BERT confirms the high performance with relatively similar macro scores overall classifiers. Only in combination with \ac{RF}, the recall and thus the f1 score is lower with \ac{BERT} compared to the other classifiers. Further, both sparse techniques count vectorizer and \ac{TF-IDF} performed quite well, especially compared to the word embedding techniques word2vec and doc2vec regardless of having additional knowledge or not for \ac{LR} and \ac{SVM}. Related to \ac{LR} and \ac{SVM} the worst performance is achieved by doc2vec. However, word2vec also performs considerably below the sparse vectors and \ac{BERT}. For example, word2vec\_I has $18\%$ less accuracy than count vectorizer for \ac{LR}. This picture is confirmed when looking at the macro table. The difference between word2vec and doc2vec compared to the sparse vectors and the \ac{BERT} vectorizer shows up more strongly. Again for \ac{LR}, in comparison with word2vec\_I, the count vectorizer performed $24\%$ better, measured by the f1 score. 

A different picture is given for \ac{RF}. The differences between the sparse methods and the word2vec techniques is much smaller and for the macro scores almost vanished, while doc2vec performs lower. Finally, compared to \ac{BERT} vectorizer, all other vectorizers are performing lower, taking the macro score as a performance metric instead of accuracy. Note that doc2vec has ill-defined scores for \ac{LR} and \ac{RF} and word2vec for \ac{SVM} since they have no predictions for some classes. Considering the formula of the macro score above, if a class has zero examples, this class is set to zero but is included in the average. Hence, the macro scores must be interpreted with caution for these vectorization techniques combined with the respective classifiers. 

The analysis of the classification algorithms confirms the vague impression in the literature. While LR and SVM have an excellent performance for the dense vectorization techniques, RF shows the best performance for the two word embedding techniques word2vec and doc2vec. For BERT, on the other hand, all classification algorithms turn out to be strong. The BERT deep learning model performed lower than BERT vectorizer but better than the other vectorization techniques. 

Concerning the last analysis dimension, the focus relies on word2vec\_II and doc2vec\_II, which contain additional knowledge for the embeddings \footnote{The \ac{BERT} model contains as well additional knowledge since search words pairs also were used for the fine-tuning. However, the model comparison is difficult since \ac{BERT} deep learning model was trained differently and allows no direct comparison}. The accuracy score in table \ref{tab: T6}  of word2vec\_II delivers no improvement by adding knowledge. For doc2vec, the results are even slightly worse, which is also reflected in the macro results. The macro results for word2vec show slightly better performance adding knowledge for \ac{SVM} and \ac{LR}, but for \ac{RF} this improvement is almost disappeared.

Table \ref{tab: T8} and \ref{tab: T10} contain the results of Level 3. Again the results of the \ac{BERT} deep learning model are reported separately in \ref{tab: T12}. A problem is that many classes only have one example. Thus, there are often zero predictions for many classes' overall classifiers and vectorization methods. This leads to ill-defined macro scores. At the same time, the accuracy still suffers from the mentioned problems of favoring classes. Nevertheless, the classifier's performance can be compared since all suffer about equally from the classes without predictions. However, not too much meaning should be given to the exact percentages.

\begin{table}[hb!]
  \center
\begin{tabular}{lrrr}
\hline
{} &   \textbf{LR} &   \textbf{SVM} &    \textbf{RF} \\
\hline
CountVectorizer &  0.50 &  0.52 &  0.44 \\
TFIDF           &  0.48 &  0.53 &  0.45 \\
Word2Vec\_I      &  0.30 &  0.15 &  0.36 \\
Word2Vec\_II     &  0.30 &  0.20 &  0.35 \\
Doc2Vec\_I       &  0.19 &  0.19 &  0.30 \\
Doc2Vec\_II      &  0.16 &  0.16 &  0.27 \\
BERT            &  0.50 &  0.46 &  0.45 \\
\hline
\end{tabular}
\caption{\label{tab: T8} Evaluation of Level 3 classification - Accuracy}
\end{table}


\begin{table}[hb!]
  \resizebox{\textwidth}{!}{%
\begin{tabular}{llll}
\hline
{} &                          \textbf{LR} &                         \textbf{SVM} &                          \textbf{RF} \\
\hline
CountVectorizer &   p: 0.41, r: 0.27, F1: 0.30 &   p: 0.40, r: 0.34, F1: 0.35 &  p: 0.36, r: 0.25, F1: 0.28 \\
TFIDF           &   p: 0.40, r: 0.23, F1: 0.27 &  p: 0.39, r: 0.33, F1: 0.35 &  p: 0.39, r: 0.26, F1: 0.29 \\
Word2Vec\_I      &  p: 0.18, r: 0.12, F1: 0.12 &  p: 0.08, r: 0.06, F1: 0.05 &   p: 0.24, r: 0.19, F1: 0.20 \\
Word2vec\_II     &  p: 0.25, r: 0.14, F1: 0.17 &   p: 0.13, r: 0.11, F1: 0.10 &   p: 0.27, r: 0.20, F1: 0.22 \\
Doc2Vec\_I       &  p: 0.11, r: 0.05, F1: 0.05 &  p: 0.12, r: 0.09, F1: 0.09 &  p: 0.22, r: 0.13, F1: 0.14 \\
Doc2Vec\_II      &  p: 0.09, r: 0.04, F1: 0.04 &  p: 0.11, r: 0.07, F1: 0.08 &  p: 0.19, r: 0.11, F1: 0.12 \\
BERT            &  p: 0.58, r: 0.51, F1: 0.53 &  p: 0.52, r: 0.48, F1: 0.48 &  p: 0.48, r: 0.33, F1: 0.36 \\
\hline
\end{tabular}
}
\caption{\label{tab: T10} Evaluation of Level 3 classification - macro}
\end{table}

In general, the performance of level 3 is lower than level 1, which is due to the higher number of classes and lower number of examples in the classes. In contrast to level 1, there is no noticeable difference in accuracy between \ac{BERT} and the sparse vectors. However, looking at the macro scores \ac{BERT} has a much higher recall, and thus a higher f1 score than the other techniques. In general, while the macro results reveal for all other techniques much worse performance compared to the accuracy score, \ac{BERT} is stable overall metrics. Thus, again it can be concluded that \ac{BERT} outperforms the other classifiers. Both sparse vectors performed relative equally. Comparing the sparse vectors against the word embeddings word2vec and doc2vec, the latter are the underdogs. Same as for level 1, doc2vec has the worst performance. 

Comparing the classifiers for level 3 shows the same picture for sparse vectors and word embeddings. Sparse vectors and \ac{BERT} vectorizer perform better with \ac{LR} and \ac{SVM}, while the word embeddings have the best result with \ac{RF}. In contrast to level 1, for \ac{BERT} the best performance is achieved by \ac{LR} on level 3, especially compared to the deep learning model, which performed much lower than the \ac{BERT} vectorization and looking at the macro results even much worse than the sparse techniques. However, concerning the results of BERT of LR and SVM, a peculiarity should be noted. During training, there was no convergence of \ac{SVM} with \ac{BERT} in a real running time. Therefore a maximal iteration of $10000$ was set. This might affect the performance of \ac{SVM}. Thus one should be careful to rank \ac{LR} as considerably better from a comparison between \ac{LR} and \ac{SVM}. 

Including additional knowledge to word2vec reveals no noticeable improvement for \ac{LR} and \ac{RF}. For \ac{SVM} there seems to be a slight improvement. The trend of macro results across all classifiers seems to confirm the improvement, although, for RF, the improvement is not too great and should not be given too much attention due to the ill-defined scores. The same problem applies to doc2vec. Roughly compared, there is no difference between doc2vec\_I and doc2vec\_II. However, the slightly poor performance of doc2vec\_II with some classification algorithms becomes apparent. 


\subsection{Deeper dive into the results}
Looking at the results, we see some trends emerging. Comparing the vectorization techniques in both Level 1 and Level 3, \ac{BERT} vectorizer has the best performance. The sparse ones perform better than the embedding techniques word2vec and do2vec for LR and SVM, while there is little difference for RF. Doc2vec performs the worst widespread. Concerning the classifiers, \ac{RF} has lower performance overall but better performance for word2vec and doc2vec compared to their results for LR and RF. While the deep learning model for level 1 has comparable but lower performance to BERT vectorization, it slips down to the performance of sparse vectors at level 3. Finally, the additional knowledge seems to lead to slightly better results for word2vec except for \ac{RF} for level 1. Doc2vec, in contrast, often shows slightly better results without adding the knowledge. Another noticeable result is that macro results are often worse than accuracy, except for BERT. All these trends require explanation. In order to get better insights and open the black box of the classifiers and the vectorization techniques at least a little bit, it is helpful to look at the actual predictions of the classifiers.


\subsubsection*{Possible explanation for Level 1 results}
For this purpose, the predictions of the test data set are used. In the first step, the predictions for all methods and classifiers are obtained. Then the distribution of the class labels is analyzed to find possible patterns. The left-hand side of figure \ref{fig: F13} shows the share of prediction labels of \ac{LR} for all vectorization techniques except the ones with additional knowledge. The highest predictions share falls on class two for all methods.
Interestingly, the share of the two embedding techniques, word2vec and docv2vec, is about the same but much higher than for other methods. Likewise, the sparse vectors form a group and have a substantially higher share than \ac{BERT}. If we look at the proportions of \ac{BERT}, the labels are more evenly distributed. There are two possible explanations for this. Either class 2 is represented in a high proportion in the test data set, and word2vec and doc2vec have led to better recognition. Alternatively, the classifiers are more or less biased towards class 2, depending on the method. The class distribution in Figure \ref{fig: F9} shows that the data is imbalanced in favor of class 2. In addition the macro scores, which capture imbalance data better, are for most models lower. Thusm it is likely that the classifiers are biased. To shed light on this, one can look at the correct predictions. The right-hand side of figure \ref{fig: F13} shows the correct predictions for each method for \ac{LR}, as well as the number of true labels for each class. At first glance, it looks like the classifiers predicted class two very well, especially compared to \ac{BERT}. However, for the other labels, the two embedding techniques, doc2vec and word2vec, have much worse performance than the other methods, indicating bias. The situation is similar for the sparse vectors compared to BERT. Obviously, the excellent performance for class two is not due to the good differentiation of class two, but to the fact that simply a very high proportion of predictions lay in class two. 

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.475\textwidth}
      \centering
      \includegraphics[width=\textwidth]{predictions_LR.jpg}
      {{\small Share of predictions labels for level 1 - \ac{LR}}
      }    
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.475\textwidth}  
      \centering 
      \includegraphics[width=\textwidth]{predictions_correct_relative_LR.jpg}
      {{\small Frequency of correct predictions for each labels and frequency of true labels - \ac{LR}}}    
  \end{subfigure}
\caption{\label{fig: F13} Confusion matrices - \ac{LR}}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.475\textwidth}
      \centering
      \includegraphics[width=\textwidth]{cm_count_LR.jpg}
      {{\small Count Vectorizer }}    
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.475\textwidth}  
      \centering 
      \includegraphics[width=\textwidth]{cm_bert_LR.jpg}
      {{\small \ac{BERT}}}    
  \end{subfigure}
  \vskip\baselineskip
  \begin{subfigure}[b]{0.475\textwidth}   
    \centering 
    \includegraphics[width=\textwidth]{cm_word2vec_without_LR.jpg}
    {{\small word2vec\_I}}    
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.475\textwidth}   
      \centering 
      \includegraphics[width=\textwidth]{cm_doc2vec_without_LR.jpg}
      {{\small doc2vec\_I}}    
  \end{subfigure}
  \caption{\label{fig: F17} Confusion matrices - \ac{LR}}
\end{figure}

In order to evidence further the bias, it is interesting to look at the predictions of class label two in comparison to the true labels. For this purpose, the confusion matrix as explained in \ref{tab: T1} can be considered. Figure \ref{fig: F17} shows the confusion matrices for all methods without additional knowledge. Also, \ac{TF-IDF} is left out because the sparse vectors behave relatively similarly. Thus it is enough to check one of the techniques. The x-axis represents the predicted labels, and the y-axis the actual labels. For doc2vec and word2vec, a vertical line is visible at the predicted label 2. This line shows that both methods often classified labels as two, although they belong to one of the other labels. At the same time, they did not do for the other labels. For doc2vec, this vertical line is even more apparent. The count vectorizer also has a stronger vertical line compared to BERT, while BERT seems to have treated all labels to the same extent. Figure \ref{fig: F17} thus again provides strong evidence for the bias. \footnote{Detecting labeling bias for a classification algorithm requires systematic analysis, as proposed in \citet{jiang2020}. This is beyond the scope of this work, so here the confusion matrix is used, which gives a good indication of whether a possible bias exists}

\ac{SVM} shows the same picture as \ac{LR}. The corresponding plots are in the appendix. In contrast, \ac{RF} behaves differently. Figure \ref{fig: F18} shows the share of predictions on the left-hand side and the frequency of correct predictions on the right-hand side. While the sparse techniques almost maintain their share of label predictions, word2vec shows a significant drop in the share of predictions for 2. It is also lower for doc2vec, but not to the same extent. Looking at the right-hand side figure, we can conclude that the sparse techniques and word2vec have the same number of predictions for almost all labels and are closer to BERT. Doc2vec classifies as already for LR label 2 very well because the share of predictions for 2 is generally higher but underestimates the other labels. From Figure 18, no clear bias can be inferred for the sparse techniques and word2vec. However, there seems to be no difference between the methods. The confusion matrices show no difference between word2vec and count vectorizer, but there is slight favoritism for label 2. Meanwhile, doc2vec shows a clear favoring of label 2. BERT remains unchanged.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.475\textwidth}
      \centering
      \includegraphics[width=\textwidth]{predictions_RF.jpg}
      {{\small Share of predictions labels for level 1 - \ac{RF}}
      }    
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.475\textwidth}  
      \centering 
      \includegraphics[width=\textwidth]{predictions_correct_relative_RF.jpg}
      {{\small Frequency of correct predictions for each labels and frequency of true labels - \ac{RF}}}    
  \end{subfigure}
\caption{\label{fig: F18} Confusion matrices Level 1- \ac{RF}}
\end{figure}


\begin{figure}
  \centering
  \begin{subfigure}[b]{0.475\textwidth}
      \centering
      \includegraphics[width=\textwidth]{cm_count_RF.jpg}
      {{\small Count Vectorizer }}    
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.475\textwidth}  
      \centering 
      \includegraphics[width=\textwidth]{cm_bert_RF.jpg}
      {{\small \ac{BERT}}}    
  \end{subfigure}
  \vskip\baselineskip
  \begin{subfigure}[b]{0.475\textwidth}   
    \centering 
    \includegraphics[width=\textwidth]{cm_word2vec_without_RF.jpg}
    {{\small word2vec\_I}}    
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.475\textwidth}   
      \centering 
      \includegraphics[width=\textwidth]{cm_doc2vec_without_RF.jpg}
      {{\small doc2vec\_I}}    
  \end{subfigure}
  \caption{\label{fig: F20} Confusion matrices Level 1 - \ac{RF}}
\end{figure}

Now the question is, how do the results of this analysis be related to performance in tables \ref{tab: T5} and \ref{tab: T6}? In light of the literature on fundamentally imbalanced data, the analysis provides a possible explanation for the disparity in performance. In their research \citet{padurariu2019} compared imbalanced text data with several vectorization and classification techniques. They conclude for small datasets that sparse techniques are not as biased as more complex techniques, like doc2vec. Furthermore, the classifiers play as well a role. Decision trees can handle imbalanced data better than linear techniques like \ac{SVM} and \ac{LR}. Systematic reviews and empirical studies confirm the better handling of imbalanced data by decision trees and \ac{RF} \citep{kaur2019, muchlinski2016, krawczyk2016}. 

In line with this literature, the analysis above shows that the sparse vectors are not as affected by the bias as word2vec and doc2vec for the linear classifiers. This is reflected in the scores of LR and of SVM in Table \ref{tab: T5} and \ref{tab: T6} and would explain the better performance compared to word2vec and doc2vec. Besides a high bias, the worst performance of doc2vec could be also due to the fact that doc2vec was not trained from a pretrained model. BERT, which is not affected by the bias, accordingly performs better. Furthermore, although the bias in RF is present, all methods are affected by it to a similar extent except doc2vec. This explains the decrease of the differences in scores between the sparse techniques and word2vec and the persistent poor performance of doc2vec. At the same time, it also explains that word2vec and doc2vec perform better overall with RF because RF can handle imbalanced data better. Since the two methods are more affected by bias overall, this has positively affected their performance. The outstanding performance of \ac{BERT} with \ac{RF} cannot be explained by the analysis above.

Besides this, the analysis does not explain the generally better performance of LR and SVM compared to RF for the dense techniques and \ac{BERT}. One would expect RF to perform better overall based on the imbalanced data. However, \ac{SVM} is said to handle large features better since they can learn independently of the dimensionality of the feature space. In addition, \ac{SVM}s are known to perform well for dense and sparse vectors, which is usually the case for text classification \citep{Joachims1998}. In addition for multiclass tasks, as mentioned in the literature review, often different versions of \ac{SVM} are used and showed good performance \citep{Aiolli2005,Angulo2003,Benabdeslem2006,Guo2015,Mayoraz1999,Tang2019,Tomar2015}. \ac{LR} LR is preferred mainly for its interpretability. A clear theoretical argument for the better performance of LR in this application cannot be found. Ultimately, the result confirms the picture of the literature. The performance of SVM and especially of \ac{LR} might be to the fact that \ac{LR} and \ac{SVM} simply work better with the data and the feature. Considering the deep learning \ac{BERT} confusion matrix in figure \ref{fig: F25}, there seems to be no bias problem, which leaves the question of why does \ac{BERT} vectorizer with \ac{LR} work better? One explanation is the difference in the size of the data. The \ac{BERT} was trained with much more data. The deep learning model probably performs better with the long dataset. However, the interpretability and transparency are lower compared to the \ac{BERT} vectorizer. As explained for the vectorizer, I controlled which pairs with which similarity are given as input to train, making it much more intuitive than the deep learning model, which automatically extracts the features. 

\begin{figure}[hb!]
  \center
  \includegraphics[scale=0.5]{cm_bert_clf_l1.jpg}
  \caption{\label{fig: F25} Covariance Matrix of \ac{BERT} - Level 1)}
\end{figure}


% The additional models for level 1 show no meaningful differences. The analysis of the covariance matrices does not show any significant results as well. For the sake of completeness, these are listed in the appendix.

Considering the last comparison perspective, it is still unexplained why doc2vec\_II performed lower, and word2vec\_II performed slightly better. \ref{fig: F23} and \ref{fig: F24} report the corresponding confusion matrices for \ac{LR} and \ac{RF} respectively. The share of predictions and the correct predictions are reported in the appendix. Considering word2vec, there are no considerable differences. Both models seem to be biased to the same extent for \ac{LR}. Same for \ac{RF}. Thus the light variation between both models might be due to the additional knowledge. In contrast, doc2vec\_II has more predictions with label $2$ belonging to other labels. Therefore, the additional knowledge in the doc2vec model might be somewhat more biased, or adding the additional knowledge is harmful to the doc2vec vectorizer. 


\begin{figure}
  \centering
  \begin{subfigure}[b]{0.475\textwidth}
      \centering
      \includegraphics[width=\textwidth]{cm_word2vec_without_LR.jpg}
      {{\small word2vec\_I \ac{LR}}}    
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.475\textwidth}   
    \centering 
    \includegraphics[width=\textwidth]{cm_word2vec_with_LR.jpg}
    {{\small word2vec\_II \ac{LR}}}    
  \end{subfigure}
  \vskip\baselineskip
  \begin{subfigure}[b]{0.475\textwidth}  
    \centering 
    \includegraphics[width=\textwidth]{cm_word2vec_without_RF.jpg}
    {{\small word2vec\_I \ac{RF}}}    
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.475\textwidth}   
      \centering 
      \includegraphics[width=\textwidth]{cm_word2vec_with_RF.jpg}
      {{\small word2vec\_II \ac{RF}}}    
  \end{subfigure}
  \caption{\label{fig: F23} Confusion matrices word2vec with and without additional knowldege Level 1}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.475\textwidth}
      \centering
      \includegraphics[width=\textwidth]{cm_doc2vec_without_LR.jpg}
      {{\small doc2vec\_I \ac{LR}}}    
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.475\textwidth}   
    \centering 
    \includegraphics[width=\textwidth]{cm_doc2vec_with_LR.jpg}
    {{\small doc2vec\_II \ac{LR}}}    
  \end{subfigure}
  \vskip\baselineskip
  \begin{subfigure}[b]{0.475\textwidth}  
    \centering 
    \includegraphics[width=\textwidth]{cm_doc2vec_without_RF.jpg}
    {{\small doc2vec\_I \ac{RF}}}    
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.475\textwidth}   
      \centering 
      \includegraphics[width=\textwidth]{cm_doc2vec_with_RF.jpg}
      {{\small doc2vec\_II \ac{RF}}}    
  \end{subfigure}
  \caption{\label{fig: F24} Confusion matrices doc2vec with and without additional knowldege Level 1}
\end{figure}


\subsubsection*{Possible explanations for Level 3 results}
Considering figure \ref{fig: F9} and the fact that level 3 reveals quite the same trends for the classifiers and methods as level 1, it is suggestible that the classifiers of level 3 suffer from the same problem. However, the considerably higher number of classes makes it challenging to examine the bias visually, as was done for level 1 for the first two analyses. Instead, only the confusion matrices are studied.

According to the trends of level 3 and the explanations for level 1, it is expected that the classifiers \ac{LR} and \ac{SVM} in combination with the sparse techniques as well as word2vec and doc2vec show a tendency towards imbalanced labels. This bias should be more apparent with doc2vec and word2vec. \ac{BERT}, in comparison, should be less biased or not biassed at all. Figure \ref{fig: F21} shows the respective confusion matrices. For readability, the class labels are removed. The predicted labels for word2vec and doc2vec indeed show clearly two vertical lines, which indicates a bias. While for count vectorizer for one label, the predictions seem as well biased, \ac{BERT} again does not suffer from the imbalance of the training data. Further comparing the confusions matrices of level 1 over all classifiers to level 3 \ac{LR} matrices, the diagonal line, except for \ac{BERT} is not visible and for doc2vec not recognizable. This shows the poor performance of the sparse techniques, word2vec and doc2vec for level 3 and, in comparison to the outstanding performance of \ac{BERT}. 


\begin{figure}
  \centering
  \begin{subfigure}[b]{0.475\textwidth}
      \centering
      \includegraphics[width=\textwidth]{cm_count_LR_3.jpg}
      {{\small Count Vectorizer }}    
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.475\textwidth}  
      \centering 
      \includegraphics[width=\textwidth]{cm_bert_LR_3.jpg}
      {{\small \ac{BERT}}}    
  \end{subfigure}
  \vskip\baselineskip
  \begin{subfigure}[b]{0.475\textwidth}   
    \centering 
    \includegraphics[width=\textwidth]{cm_word2vec_without_LR_3.jpg}
    {{\small word2vec\_I}}    
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.475\textwidth}   
      \centering 
      \includegraphics[width=\textwidth]{cm_doc2vec_without_LR_3.jpg}
      {{\small doc2vec\_I}}    
  \end{subfigure}
  \caption{\label{fig: F21} Confusion matrices Level 3- \ac{LR}}
\end{figure}


\begin{figure}
  \centering
  \begin{subfigure}[b]{0.475\textwidth}
      \centering
      \includegraphics[width=\textwidth]{cm_count_RF_3.jpg}
      {{\small Count Vectorizer }}    
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.475\textwidth}  
      \centering 
      \includegraphics[width=\textwidth]{cm_bert_RF_3.jpg}
      {{\small \ac{BERT}}}    
  \end{subfigure}
  \vskip\baselineskip
  \begin{subfigure}[b]{0.475\textwidth}   
    \centering 
    \includegraphics[width=\textwidth]{cm_word2vec_without_RF_3.jpg}
    {{\small word2vec\_I}}    
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.475\textwidth}   
      \centering 
      \includegraphics[width=\textwidth]{cm_doc2vec_without_RF_3.jpg}
      {{\small doc2vec\_I}}    
  \end{subfigure}
  \caption{\label{fig: F22} Confusion matrices Level 3- \ac{RF}}
\end{figure}

Similar to level 1, the performance for \ac{RF} is, due to the better handling of imbalance, better for word2vec and doc2vec, which is reflected in their confusion matrices in figure \ref{fig: F22}. While the performance of word2vec and doc2vec are closer to the sparse techniques for \ac{RF} than for the linear techniques, the confusions matrices do not clearly show differences. In addition, it is interesting that \ac{BERT} has a considerably better performance with \ac{RF}, while the confusions matrices do not have substantial differences in terms of the bias. Thus, the differences between the results for \ac{RF} remain unexplained by the analysis.

As for level 1, the generally lower performance for \ac{RF} for \ac{BERT} and sparse vectors cannot be explained by the analysis and is probably because \ac{RF} is not as suitable for the application as \ac{LR} and \ac{SVM}. More interestingly is the low performance of the deep learning model. Figure \ref{fig: F26}, however, shows no differences. As mentioned above, for level 1, the deep learning model trained with much lower data might have a greater impact on a classification problem with much more labels. The pairs for the vectorization are also tailored specifically for level 3. Following the literature, this might be a better strategy than using a deep learning model with automatic feature selection for a small dataset. 

The additional knowledge models behave similarly to level 1. The confusion matrices do not provide any remarkable differences. The additional knowledge seems to help word2vec perform slightly better, while for doc2vec, this seems harmful. 

\begin{figure}[hb!]
  \center
  \includegraphics[scale=0.5]{cm_bert_clf_3.jpg}
  \caption{\label{fig: F26} Confusion matrix \ac{BERT} deep learning model - Level 3}
\end{figure}


\section{Conclusion and Limitations}
The starting point of this work was to examine the classification of job titles of German job postings with the taxonomy \ac{KldB} 2010. Based on the literature on text classification and the challenges of short text classification, the analysis was guided by three pillars: Application of different vectorization techniques, training of different classification algorithms, and treatment of short job titles by adding additional knowledge. 

The results revealed some interesting trends. \ac{BERT} vectorization technique in combination with \ac{LR} has the best performance. Due to imbalanced data, the other classification algorithms have developed a label bias in different settings, both for level 1 and level 3. \ac{RF} can reduce this bias for Doc2vec and Word2vec, but not to the extent that it overpowers the sparse vectors or BERT results, especially in combination with the linear classification algorithms. The deep learning model of \ac{BERT} performs in general and especially for level 3, worse than \ac{BERT} vectorizer. Adding additional knowledge does not give clear improvement. The word2vec performance seems to improve slightly, while that of doc2vec deteriorates slightly. 

It can be concluded from this study that due to the stability of the \ac{BERT} algorithm against the imbalanced data, as well as over all classification algorithms, the BERT vectorizer in combination with LR is a promising method for the classification of job titles into the ``kldb" classes. The possibility of controlling which pairs are used as input for the fine-tuning gives the \ac{BERT} vectorizer the advantage of being interpretable and transparent. Furthermore, there is much space for improvements with this vectorizer method. Future research might combine pairs in other ways, like including the label names of the ``kldbs" or combine search words for different levels with different probabilities. This produces vast datasets with millions of pairs for the fine-tuning process. 

The results in this paper contribute to the literature because it provides initial results and first analysis on job title classification in Germany. The presented results on \ac{BERT} and the problem of biased classifiers are leading the way for future research. Nevertheless, some limitations require future research to develop a high-performance classification algorithm. The first limitation concerns the dataset. As detailed in the discussion of the results, the data imbalance has been problematic for most methods. This was reflected in the performance, especially for level 3, where many classes have few to no examples. Due to this fact, it was also not possible to train all taxonomy classes. 8 out of the 144 classes on level 3 were not trained. For level 1, class 0 could not be trained. In addition, this problem led to ill-defined metrics, which allowed partly only a cautious interpretation of the performance. Further, regarding the dataset, only a small version of the dataset could be used in this work due to computational resources. It is expected that a larger dataset could improve the results, especially for the deep learning model. It will be important that future research investigate how to cope with the imbalance of the data in order to compare the vectorization methods again without the bias problem. Another strategy would be to enrich the data manually with examples for minority classes. This would also be important to ensure the training of all classes.  

The second limitation addresses the question of generalization. Often the problem is that the classifiers are overfitted to the training data. The data was divided into training and test data to ensure that the performance was measured on unseen data. However, since not all classes are trained, the classifiers cannot classify titles belonging to those missing classes, restricting the generalization of the algorithm. In addition, the classifiers are not multilingual and thus not generalizable to other job postings than German.  

Lastly, concerning the additional knowledge, there is more way for improvement. As shown in the literature, there are several techniques for dealing with short texts except for including additional knowledge in the way it was done in this study. As \ac{BERT} is a promising technique, one could combine and compare different \ac{BERT} algorithms with additional knowledge. For example, \citet{ostendorff2019} use an approach of enriching \ac{BERT} with knowledge graph embeddings. This is an attractive topic for future work. 

One last point regarding ``kldbs" should be stated. Considering the occupation software developer, figure \ref{fig: F27} shows the ``kldbs" that are assigned to the title with the terms ``softwareentwickler" or ``softwareentwicklerin" in the training dataset. Most of the titles belong to ``434", but approx $30\%$ are assigned to other kldbs. Comparing the titles in the different kldbs besides the word `softwareentwickler", most of them do not contain other keywords which would differentiate between the kldbs, they are the same, or are consist of the word `softwareentwickler". Thus, the data is ambiguous for some occupations and ``kldbs" making it impossible for a classifier to differentiate between the kldbs. This reveals the complexity of the problem and the limitations of improvements in the performance. 

\begin{figure}[hb!]
  \center
  \includegraphics[scale=0.5]{kldbs_frequency_softwareentwickler.jpg}
  \caption{\label{fig: F27} Share of kldbs for the occupation `softwareentwickler"}
\end{figure}

The problem just highlighted raises the question of whether the limitation is purely a data set quality issue. This can be clarified by analyzing the alternative ``kldbs" that employers can provide in case of uncertainty. The heatmap in \ref{fig: F28} shows on the horizontal line the kldbs which were used for the training and on the vertical line the alternative kldbs. Considering again the main kldb $431$, the vertical line that emanates from the kldb indicates that it is not clear at all what kldb a title might belong in the real world. \footnote{Note that this analysis is on the long data set, not on the short one. This is due to illustration purposes of highlighting the problem better. However, since a sample with the same distribution was drawn, the logic is the same.} This raises the question of whether it is adequate to develop a classifier with only one class as an output. At the same time, one should elaborate on whether a multilabel classification algorithm is practical anymore for downstream tasks of this domain. In addition, there are concerns about appropriate evaluation measurements. This may constitute the object of future studies.

\begin{figure}[hb!]
  \center
  \includegraphics[scale=0.5]{co_occurence_softwareentwickler.jpg}
  \caption{\label{fig: F28} Co-occurence matrix for the occupation `softwareentwickler"}
\end{figure}

% \section{sentences}
% \citet{ajose2020} concludes that for there are different results for different classifiers. 


\clearpage


\bibliographystyle{apalike}
\bibliography{export}

\newpage
\appendix
\section{Data}
\subsection{Data snippet raw data}
\begin{lstlisting}[language=json]
  {
    "hashId": "-IgNS05-jeri5aZhe0_VK35Y0-6xQAoADg3b0MyraTI=",
    "hauptberuf": "Telefonist/in",
    "freieBezeichnung": "Telefonist / Telefonistin m/w/d",
    "referenznummer": "14469-20210617140207-S",
    "mehrereArbeitsorteVorhanden": false,
    "arbeitgeber": "aventa Personalmanagement GmbH",
    "arbeitgeberHashId": "MYRG2meMKxCjrQ9Cpl8JwgEDPbM133Z9iRCkolaOONo=",
    "aktuelleVeroeffentlichungsdatum": "2021-06-29",
    "eintrittsdatum": "2021-06-29",
    "logoHashId": "wMN78p7yNK_C0aJDJ77l63RVH3DCEzwJGxZk1ZzsUrY=",
    "angebotsart": "ARBEIT",
    "hauptDkz": "7389",
    "alternativDkzs": [
      "35082"
    ],
    "angebotsartGruppe": "ARBEIT",
    "anzeigeAnonym": false,
    "arbeitsort": {
      "plz": "10407",
      "ort": "Berlin",
      "region": "Berlin",
      "land": "Deutschland",
      "koordinaten": {
        "lat": 52.5335379,
        "lon": 13.4462856
      }
    },
    "_links": {
      "details": {
        "href": "http://jobboerse.arbeitsagentur.de/vamJB/stellenangebotAnzeigen.html?bencs=xZ8NQKDByg2g6avJgLLIrGwqlXZQi1GKNAI%2BzAoCWJ5RD6egZDnwqMFj%2B4AnUX6XN5nyEJ7NKSdBBr1EvlmnVw%3D%3D"
      },
      "arbeitgeberlogo": {
        "href": "https://api-con.arbeitsagentur.de/prod/jobboerse/jobsuche-service/ed/v1/arbeitgeberlogo/wMN78p7yNK_C0aJDJ77l63RVH3DCEzwJGxZk1ZzsUrY="
      },
      "jobdetails": {
        "href": "https://api-con.arbeitsagentur.de/prod/jobboerse/jobsuche-service/pc/v1/jobdetails/-IgNS05-jeri5aZhe0_VK35Y0-6xQAoADg3b0MyraTI="
      }
    }
  },
\end{lstlisting}


\subsection{Trainingsdata snippet (without preprocessing) - Level 1}
\begin{lstlisting}[language=json, firstnumber=1]
{'id': '2', 'title': 'Maschinenbediener (m/w/d)'}
{'id': '7', 'title': 'Controlling'}
{'id': '5', 'title': 'Lagermitarbeiter (m/w/d)'}
{'id': '2', 'title': 'Reifenmonteur (m/w/d) Facharbeiter'}
{'id': '5', 'title': 'Kommissionierer (m /w /d)'}
{'id': '7', 'title': 'Sachbearbeiter (m/w/d) im Einkauf Weimar'}
{'id': '5', 'title': 'Schubmaststapler Fahrer (m/w/d)'}
{'id': '3', 'title': 'Bauhelfer Elektroinstallation (m/w/d)'}
{'id': '7', 'title': 'Telefonist / Telefonistin m/w/d'}
{'id': '9', 'title': 'Telefonische Kundenbetreuung (m/w/d)'}
\end{lstlisting}


\subsection{Trainingdata snippted (preprocessed) - Level 1}
\begin{lstlisting}[language=json, firstnumber=1]
[{'id': '2', 'title': 'maschinenbediener'},
 {'id': '7', 'title': 'controlling'},
 {'id': '5', 'title': 'lagermitarbeiter'},
 {'id': '2', 'title': 'reifenmonteur facharbeiter'},
 {'id': '5', 'title': 'kommissionierer'},
 {'id': '7', 'title': 'sachbearbeiter einkauf weimar'},
 {'id': '5', 'title': 'schubmaststapler fahrer'},
 {'id': '3', 'title': 'bauhelfer elektroinstallation'},
 {'id': '7', 'title': 'telefonist telefonistin'},
 {'id': '9', 'title': 'telefonische kundenbetreuung'}]
\end{lstlisting}

\subsection{Class distribution of level 1 und level 3}
\begin{figure}[hb!]
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{training_data_long_L1.png}
    \caption{\label{fig: F11} Class distribution Level 1}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{training_data_long_L3.png}
    \caption{\label{fig: F12} Class distribution Level 3}
  \end{subfigure}
  \caption{Class distribution of training data}
  \end{figure}

% \section{Naive Bayes Classifier}
% \subsection{Theory}
% The \ac{NB}, a family of probabilistic classifiers, uses Bayes' rule in order to determine the most likely class for each document \citep{Schneider2005}. All \ac{NB} classifiers rely on the conditional independence assumptions which means, that ``features are independent of each other, given the category variable'' \citep[48]{Xu2018}. Depending on whether the features are discrete or continous, different distributions, so-called event models are proposed. While from a theoretical perspective for continous features Gaussian distribution is well-suited, for discrete features usually Bernoulli or multinomial distibutions are applied \citep{Xu2018}. Although, popular practical implementations, like the one from sklearn, allow as well fractional counts for multinomial distributions \citep{scikit-learn}. Trying different event models, the multinomial \ac{NB} shows indeed for both Count Vectorizer and for TFIDF the best results, which is why I choose it as event model for the baseline. 

% The multinomial \ac{NB} classifies according to the most likely class. Given that a document d has $t = 1, ...., k$ terms and can be assigned to $c = 1,...,j$ classes, the probability of a term in a document given a class is calculated as \citep{Manning2008}:
% \[ P(t_k, c_j) = \frac{occurence(t_k, c_j) + 1}{\sum occurence(t, c_j) + |V|} \]

% where $|V|$ is the cardinality of the vocabulary. In the denominator 1 is added, so-called Laplace smoothing, in order to avoid zeros, which is the case if the number of terms in a document for one class is zero. \citep{Manning2008}. Further given that $N_c = count(c_j)$ is the number of documents belonging to class $c_j$ and N is number of documents the probability of $c_j$ is defined as $\frac{N_c}{N}$. The probability of a document d belonging to a class $c_j$ can then formulated as follows \citep[258]{Manning2008}:
% \[ P(c_j|d) \propto P(c_j) \prod_{i = 1}^k P(t_i|c_j) \]
% Then the most likely classes can be determined by \citep{Manning2008}: 
% \[\argmax_{c \in C} P(c_j) \prod_{i = 1}^k P(t_i|c_j) \] 

\section{Results}



\end{document}
