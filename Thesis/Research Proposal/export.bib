@article{Le2006,
  author    = {Quoc V Le and Alex J Smola},
  city      = {New York, New York, USA},
  doi       = {10.1145/1143844},
  journal   = {Proceedings of the 23rd international conference on Machine learning  - ICML '06},
  publisher = {ACM Press},
  title     = {Simpler Knowledge-based Support Vector Machines},
  year      = {2006}
}
@article{WangJ2017,
  author  = {Jin Wang and Zhongyuan Wang and Dawei Zhang and Jun Yan},
  title   = {Combining Knowledge with Deep Convolutional Neural Networks for Short Text Classification},
  url     = {https://concept.msra.cn/},
  year    = {2017},
  journal = {Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence}
}
@article{Japkowicz2000,
  author  = {Nathalie Japkowicz},
  title   = {Learning from Imbalanced Data Sets: A Comparison of Various Strategies},
  url     = {www.aaai.org},
  year    = {2000},
  journal = {AAAI Technical Report WS-00-05}
}
@article{Neculoiu2016,
  author  = {Paul Neculoiu and Maarten Versteegh and Mihai Rotaru and Textkernel B V Amsterdam},
  pages   = {148-157},
  title   = {Learning Text Similarity with Siamese Recurrent Networks},
  year    = {2016},
  journal = {Association for Computational Linguistics}
}
@article{Martin2021,
  author    = {Ananda Martin-Caughey},
  doi       = {10.1177/00031224211042053},
  issue     = {5},
  journal   = {American Social Review},
  month     = {9},
  pages     = {960-999},
  publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
  title     = {What’s in an Occupation? Investigating Within-Occupation Variation and Gender Segregation Using Job Titles and Task Descriptions:},
  volume    = {86},
  year      = {2021}
}
@article{Bodyston2019,
  author    = {Paige S. Boydston and Erica S. Jowett Hirst},
  issue     = {2},
  journal   = {Behavior Analysis in Practice 2019 13:2},
  keywords  = {Psychology,general},
  month     = {8},
  pages     = {394-401},
  publisher = {Springer},
  title     = {Public Perceptions and Understanding of Job Titles Related to Behavior Analysis},
  volume    = {13},
  year      = {2019}
}
@article{Li2021,
  author    = {Lei Li and Svetlana Peltsverger and Jack Zheng and Linh Le and Michael Handlin},
  journal   = {SIGITE '21: Proceedings of the 22st Annual Conference on Information Technology Education},
  pages     = {85-90},
  publisher = {Association for Computing Machinery (ACM)},
  title     = {Retrieving and Classifying LinkedIn Job Titles for Alumni Career Analysis},
  year      = {2021}
}
@article{Smith1989,
  author    = {Brien N. Smith and Jeffrey S. Hornsby and Philip G. Benson and Mark Wesolowski},
  issue     = {3},
  journal   = {Journal of Business and Psychology},
  pages     = {341-351},
  publisher = {Springer},
  title     = {What is in a name: The impact of job titles on job evaluation results},
  volume    = {3},
  year      = {1989}
}
@article{Bohm2020,
  author    = {Stephan Böhm and Olena Linnyk and Jens Kohl and Tim Weber and Ingolf Teetz and Katarzyna Bandurka and Martin Kersting Justus},
  journal   = {Proceedings of the 2020 on Computers and People Research Conference},
  keywords  = {Gender bias,job postings,text analysis},
  publisher = {ACM},
  title     = {Analysing Gender Bias in IT Job Postings: A Pre-Study Based on Samples from the German Job Market},
  year      = {2020}
}
@article{Paulus2013,
  author    = {Wiebke Paulus and Britta Matthes},
  journal   = {FDZ Methodenreport},
  publisher = {Institut für Arbeitsmarkt- und Berufsforschung (IAB), Nürnberg [Institute for Employment Research, Nuremberg, Germany]},
  title     = {Klassifikation der Berufe : Struktur, Codierung und Umsteigeschlüssel},
  year      = {2013}
}
@article{WangH2020,
  abstract  = {In order to solve the problem that traditional short text classification methods do not perform well on short text due to the data sparsity and insufficient semantic features, we propose a short text classification method based on convolutional neural network and semantic extension. Firstly, we propose an improved similarity to improve the coverage of the word vector table in the short text preprocessing process. Secondly, we propose a method for semantic expansion of short texts, which adding an attention mechanism to the neural network model to find related words in the short text, and semantic expansion is performed at the sentence level and the related word level of the short text respectively. Finally, the feature extraction of short text is carried out by means of the classical convolutional neural network. The experimental results show that the proposed method is feasible during the classification task of short text, and the classification effectiveness is significantly improved.},
  author    = {Haitao Wang and Keke Tian and Zhengjiang Wu and Lei Wang},
  issue     = {1},
  journal   = {International Journal of Computational Intelligence Systems},
  pages     = {367-375},
  publisher = {Atlantis Press},
  title     = {A Short Text Classification Method Based on Convolutional Neural Network and Semantic Extension},
  volume    = {14},
  year      = {2020}
}
@article{WangC2016,
  author  = {Chenguang Wang and Yangqiu Song and Haoran Li and Ming Zhang and Jiawei Han},
  journal = {Thirtieth AAAI Conference on Artificial Intelligence},
  title   = {Text Classification with Heterogeneous Information Network Kernels},
  year    = {2016}
}
@article{Jacovi2018,
  author    = {Alon Jacovi and Oren Sar Shalom and Yoav Goldberg},
  journal   = {arXiv preprint arXiv},
  pages     = {56-65},
  publisher = {Association for Computational Linguistics (ACL)},
  title     = {Understanding Convolutional Neural Networks for Text Classification},
  year      = {2018}
}
@article{Cai2019,
  journal   = {2018 15th International Computer Conference on Wavelet Active Media Technology and Information Processing, ICCWAMTIP 2018},
  pages     = {123-126},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  title     = {Deeplearning Model Used in Text Classification},
  year      = {2019}
}
@article{Gosh2018,
  author    = {Samujjwal Ghosh and Maunendra Sankar Desarkar},
  journal   = {The Web Conference 2018 - Companion of the World Wide Web Conference, WWW 2018},
  pages     = {1629-1637},
  publisher = {Association for Computing Machinery, Inc},
  title     = {Class Specific TF-IDF Boosting for Short-text Classification: Application to Short-texts Generated during Disasters},
  year      = {2018}
}
@article{Marivate2020,
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  pages     = {385-399},
  publisher = {Springer, Cham},
  title     = {Improving Short Text Classification Through Global Augmentation Methods},
  volume    = {12279 LNCS},
  year      = {2020}
}
@article{ZhangH2016,
  author    = {Heng Zhang and Guoqiang Zhong},
  journal   = {Knowledge-Based Systems},
  keywords  = {Data enrich,Short texts,Topic model,Word and topic vectors},
  pages     = {76-86},
  publisher = {Elsevier},
  title     = {Improving short text classification by learning vector representations of both words and hidden topics},
  volume    = {102},
  year      = {2016}
}
@article{Bouaziz2014,
  author    = {Ameni Bouaziz and Christel Dartigues-Pallez and Célia da Costa Pereira and Frédéric Precioso and Patrick Lloret},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  pages     = {288-299},
  publisher = {Springer, Cham},
  title     = {Short Text Classification Using Semantic Random Forest},
  volume    = {8646 LNCS},
  year      = {2014}
}
@article{Zhou2010,
  author   = {Faguo Zhou and Fan Zhang and Bingru Yang and Xingang Yu},
  doi      = {10.1109/ISECS.2010.9},
  journal  = {3rd International Symposium on Electronic Commerce and Security, ISECS 2010},
  keywords = {Feature extraction,Rules,Short text,Short text classification,Statistics},
  pages    = {3-7},
  title    = {Research on short text classification algorithm based on statistics and rules},
  year     = {2010}
}
@article{Alsmadi2019,
  abstract  = {Rapid developments in social networks and their usage in everyday life have caused an explosion in the amount of short electronic documents. Thus, the need to classify this type of document based on their content has a significant implication in many applications. The need to classify these documents in relevant classes according to their text contents should be interested in many practical reasons. Short-text classification is an essential step in many applications, such as spam filtering, sentiment analysis, Twitter personalization, customer review and many other applications related to social networks. Reviews on short text and its application are limited. Thus, this paper aims to discuss the characteristics of short text, its challenges and difficulties in classification. The paper attempt to introduce all stages in principle classification, the technique used in each stage and the possible development trend in each stage.,The paper as a review of the main aspect of short-text classification. The paper is structured based on the classification task stage.,This paper discusses related issues and approaches to these problems. Further research could be conducted to address the challenges in short texts and avoid poor accuracy in classification. Problems in low performance can be solved by using optimized solutions, such as genetic algorithms that are powerful in enhancing the quality of selected features. Soft computing solution has a fuzzy logic that makes short-text problems a promising area of research.,Using a powerful short-text classification method significantly affects many applications in terms of efficiency enhancement. Current solutions still have low performance, implying the need for improvement. This paper discusses related issues and approaches to these problems.},
  author    = {Issa Alsmadi and Keng Hoon Gan},
  issue     = {2},
  journal   = {International Journal of Web Information Systems},
  keywords  = {Classification,Feature selection,Sentiment analysis,Short text,Social networks},
  month     = {6},
  pages     = {155-182},
  publisher = {Emerald Publishing Limited},
  title     = {Review of short-text classification},
  volume    = {15},
  year      = {2019}
}
@article{Yu2013,
  abstract = {LibShortText is an open source library for short-text classification and analysis. Properties of short texts are considered in its design and implementation. The package supports effective text pre-processing and fast training/prediction procedures. We provide an interactive tool to perform error analysis. Because of the short length, details of each short text can be investigated easily. In addition, the package is designed so that users can conveniently make extensions. The ease of use, efficiency, and extensibility of LibShortText make it very useful for practitioners working on short-text classification and analysis. The package is available at},
  author   = {Hsiang-Fu Yu and Chia-Hua Ho and Yu-Chin Juan and Chih-Jen Lin},
  journal  = {Department of Computer Science, National Taiwan University},
  keywords = {interactive error analysis,linear classi-fication,machine learning,open source,short-text classification},
  pages    = {1--5},
  title    = {LibShortText: A Library for Short-text Classification and Analysis},
  url      = {http://www.csie.ntu.edu.tw/~cjlin/libshorttext},
  year     = {2013}
}
@article{Srklj2021,
  abstract  = {The use of background knowledge is largely unexploited in text classification tasks. This paper explores word taxonomies as means for constructing new semantic features, which may improve the performance and robustness of the learned classifiers. We propose tax2vec, a parallel algorithm for constructing taxonomy-based features, and demonstrate its use on six short text classification problems: prediction of gender, personality type, age, news topics, drug side effects and drug effectiveness. The constructed semantic features, in combination with fast linear classifiers, tested against strong baselines such as hierarchical attention neural networks, achieves comparable classification results on short text documents. The algorithm's performance is also tested in a few-shot learning setting, indicating that the inclusion of semantic features can improve the performance in data-scarce situations. The tax2vec capability to extract corpus-specific semantic keywords is also demonstrated. Finally, we investigate the semantic space of potential features, where we observe a similarity with the well known Zipf's law.},
  author    = {Blaž Škrlj and Matej Martinc and Jan Kralj and Nada Lavrač and Senja Pollak},
  doi       = {10.1016/J.CSL.2020.101104},
  journal   = {Computer Speech & Language},
  keywords  = {feature construction,semantic enrichment,short documents,taxonomies,text classification,vectorization},
  month     = {1},
  pages     = {101104},
  publisher = {Academic Press},
  title     = {tax2vec: Constructing Interpretable Features from Taxonomies for Short Text Classification},
  volume    = {65},
  year      = {2021}
}
@article{Liu2005,
  abstract  = {Very large-scale classification taxonomies typically have hundreds of thousands of categories, deep hierarchies, and skewed category distribution over documents. However, it is still an open question whether the state-of-the-art technologies in automated text categorization can scale to (and perform well on) such large taxonomies. In this paper, we report the first evaluation of Support Vector Machines (SVMs) in web-page classification over the full taxonomy of the Yahoo! categories. Our accomplishments include: 1) a data analysis on the Yahoo! taxonomy; 2) the development of a scalable system for large-scale text categorization; 3) theoretical analysis and experimental evaluation of SVMs in hierarchical and non-hierarchical settings for classification; 4) an investigation of threshold tuning algorithms with respect to time complexity and their effect on the classification accuracy of SVMs. We found that, in terms of scalability, the hierarchical use of SVMs is efficient enough for very large-scale classification; however, in terms of effectiveness, the performance of SVMs over the Yahoo! Directory is still far from satisfactory, which indicates that more substantial investigation is needed.},
  author    = {Tie-Yan Liu and Yiming Yang and Hao Wan and Hua-Jun Zeng and Zheng Chen and Wei-Ying Ma},
  doi       = {10.1145/1089815.1089821},
  issue     = {1},
  journal   = {ACM SIGKDD Explorations Newsletter},
  month     = {6},
  pages     = {36-43},
  publisher = {Association for Computing Machinery (ACM)},
  title     = {Support vector machines classification with a very large-scale taxonomy},
  volume    = {7},
  year      = {2005}
}
@article{Joulin207,
  abstract  = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute.},
  author    = {Armand Joulin and Edouard Grave and Piotr Bojanowski and Tomas Mikolov},
  journal   = {15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017 - Proceedings of Conference},
  pages     = {427-431},
  publisher = {Association for Computational Linguistics (ACL)},
  title     = {Bag of tricks for efficient text classification},
  volume    = {2},
  year      = {2017}
}
@article{Liu2017,
  abstract  = {Extreme multi-label text classification (XMTC) refers to the problem of assigning to each document its most relevant subset of class labels from an extremely large label collection, where the number of labels could reach hundreds of thousands or millions. The huge label space raises research challenges such as data sparsity and scalability. Significant progress has been made in recent years by the development of new machine learning methods, such as tree induction with large-margin partitions of the instance spaces and label-vector embedding in the target space. However, deep learning has not been explored for XMTC, despite its big successes in other related areas. This paper presents the first attempt at applying deep learning to XMTC, with a family of new Convolutional Neural Network (CNN) models which are tailored for multi-label classification in particular. With a comparative evaluation of 7 state-of-The-Art methods on 6 benchmark datasets where the number of labels is up to 670,000, we show that the proposed CNN approach successfully scaled to the largest datasets, and consistently produced the best or the second best results on all the datasets. On the Wikipedia dataset with over 2 million documents and 500,000 labels in particular, it outperformed the second best method by 11:7% ∼ 15:3% in precision@K and by 11:5% ∼ 11:7% in NDCG@K for K = 1,3,5.},
  author    = {Jingzhou Liu and Wei Cheng Chang and Yuexin Wu and Yiming Yang},
  doi       = {10.1145/3077136.3080834},
  journal   = {SIGIR 2017 - Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  month     = {8},
  pages     = {115-124},
  publisher = {Association for Computing Machinery, Inc},
  title     = {Deep learning for extreme multi-label text classification},
  year      = {2017}
}
@article{ZhangR2016,
  abstract  = {The goal of sentence and document modeling is to accurately represent the meaning of sentences and documents for various Natural Language Processing tasks. In this work, we present Dependency Sensitive Convolutional Neural Networks (DSCNN) as a generalpurpose classification system for both sentences and documents. DSCNN hierarchically builds textual representations by processing pretrained word embeddings via Long Short- Term Memory networks and subsequently extracting features with convolution operators. Compared with existing recursive neural models with tree structures, DSCNN does not rely on parsers and expensive phrase labeling, and thus is not restricted to sentencelevel tasks. Moreover, unlike other CNNbased models that analyze sentences locally by sliding windows, our system captures both the dependency information within each sentence and relationships across sentences in the same document. Experiment results demonstrate that our approach is achieving state-ofthe-art performance on several tasks, including sentiment analysis, question type classification, and subjectivity classification.},
  author    = {Rui Zhang and Honglak Lee and Dragomir Radev},
  journal   = {2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference},
  pages     = {1512-1521},
  publisher = {Association for Computational Linguistics (ACL)},
  title     = {Dependency sensitive convolutional neural networks for modeling sentences and documents},
  year      = {2016}
}
@article{Conneau2016,
  abstract = {The dominant approach for many NLP tasks are recurrent neural networks, in
              particular LSTMs, and convolutional neural networks. However, these
              architectures are rather shallow in comparison to the deep convolutional
              networks which have pushed the state-of-the-art in computer vision. We present
              a new architecture (VDCNN) for text processing which operates directly at the
              character level and uses only small convolutions and pooling operations. We are
              able to show that the performance of this model increases with depth: using up
              to 29 convolutional layers, we report improvements over the state-of-the-art on
              several public text classification tasks. To the best of our knowledge, this is
              the first time that very deep convolutional nets have been applied to text
              processing.},
  author   = {Alexis Conneau and Holger Schwenk and Loïc Barrault and Yann Lecun},
  journal  = {Nature},
  month    = {6},
  pages    = {1-11},
  title    = {Very Deep Convolutional Networks for Text Classification},
  url      = {https://arxiv.org/abs/1606.01781v2},
  year     = {2016}
}
@article{ZhangX2015,
  abstract = {This article offers an empirical exploration on the use of character-level convolu-tional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},
  author   = {Xiang Zhang and Junbo Zhao and Yann Lecun},
  isbn     = {0123456789},
  journal  = {Advances in neural information processing systems },
  pages    = {649-657},
  title    = {Character-level Convolutional Networks for Text Classification *},
  volume   = {28},
  year     = {2015}
}
@article{WangP2016,
  abstract  = {Text classification can help users to effectively handle and exploit useful information hidden in large-scale documents. However, the sparsity of data and the semantic sensitivity to context often hinder the classification performance of short texts. In order to overcome the weakness, we propose a unified framework to expand short texts based on word embedding clustering and convolutional neural network (CNN). Empirically, the semantically related words are usually close to each other in embedding spaces. Thus, we first discover semantic cliques via fast clustering. Then, by using additive composition over word embeddings from context with variable window width, the representations of multi-scale semantic units. 11Semantic units are defined as n-grams which have dominant meaning of text. With n varying, multi-scale contextual information can be exploited. in short texts are computed. In embedding spaces, the restricted nearest word embeddings (NWEs). 22In order to prevent outliers, a Euclidean distance threshold is preset between semantic cliques and semantic units, which is used as restricted condition. of the semantic units are chosen to constitute expanded matrices, where the semantic cliques are used as supervision information. Finally, for a short text, the projected matrix. 33The projected matrix is obtained by table looking up, which encodes Unigram level features. and expanded matrices are combined and fed into CNN in parallel. Experimental results on two open benchmarks validate the effectiveness of the proposed method.},
  author    = {Peng Wang and Bo Xu and Jiaming Xu and Guanhua Tian and Cheng Lin Liu and Hongwei Hao},
  doi       = {10.1016/J.NEUCOM.2015.09.096},
  journal   = {Neurocomputing},
  keywords  = {Classification,Clustering,Convolutional neural network,Semantic units,Short text,Word embeddings},
  month     = {1},
  pages     = {806-814},
  publisher = {Elsevier},
  title     = {Semantic expansion using word embedding clustering and convolutional neural network for improving short text classification},
  volume    = {174},
  year      = {2016}
}
@article{Yao2019,
  abstract = {Text classification is an important and classical problem in natural language processing. There have been a number of studies that applied convolutional neural networks (convolution on regular grid, e.g., sequence) to classification. However, only a limited number of studies have explored the more flexible graph convolutional neural networks (convolution on non-grid, e.g., arbitrary graph) for the task. In this work, we propose to use graph convolutional networks for text classification. We build a single text graph for a corpus based on word co-occurrence and document word relations, then learn a Text Graph Convolutional Network (Text GCN) for the corpus. Our Text GCN is initialized with one-hot representation for word and document, it then jointly learns the embeddings for both words and documents, as supervised by the known class labels for documents. Our experimental results on multiple benchmark datasets demonstrate that a vanilla Text GCN without any external word embeddings or knowledge outperforms state-of-the-art methods for text classification. On the other hand, Text GCN also learns predictive word and document embeddings. In addition, experimental results show that the improvement of Text GCN over state-of-the-art comparison methods become more prominent as we lower the percentage of training data, suggesting the robustness of Text GCN to less training data in text classification.},
  author   = {Liang Yao and Chengsheng Mao and Yuan Luo},
  doi      = {10.1609/AAAI.V33I01.33017370},
  issue    = {01},
  journal  = {Proceedings of the AAAI Conference on Artificial Intelligence},
  month    = {7},
  pages    = {7370-7377},
  title    = {Graph Convolutional Networks for Text Classification},
  volume   = {33},
  url      = {https://ojs.aaai.org/index.php/AAAI/article/view/4725},
  year     = {2019}
}
@article{Decorte2021,
  abstract = {Job titles form a cornerstone of today's human resources (HR) processes.
              Within online recruitment, they allow candidates to understand the contents of
              a vacancy at a glance, while internal HR departments use them to organize and
              structure many of their processes. As job titles are a compact, convenient, and
              readily available data source, modeling them with high accuracy can greatly
              benefit many HR tech applications. In this paper, we propose a neural
              representation model for job titles, by augmenting a pre-trained language model
              with co-occurrence information from skill labels extracted from vacancies. Our
              JobBERT method leads to considerable improvements compared to using generic
              sentence encoders, for the task of job title normalization, for which we
              release a new evaluation benchmark.},
  author   = {Jens-Joris Decorte and Jeroen Van Hautte and Thomas Demeester and Chris Develder},
  journal  = {arXiv preprint arXiv},
  keywords = {Extraction,Infor-mation,Job,Normalization ·,Semantic,Similarity ·,Text,Title},
  month    = {9},
  pages    = {1-9},
  title    = {JobBERT: Understanding Job Titles through Skills},
  url      = {https://arxiv.org/abs/2109.09605v1},
  year     = {2021}
}
@article{Marinescu2020,
  abstract  = {On the leading job board CareerBuilder.com, high-wage job postings unexpectedly attract fewer applicants, and this is the case even within a detailed occupation. Viewed through the lens of our directed search model, this negative relationship is indicative of substantial applicant heterogeneity within an occupation. Empirically, we find that job title heterogeneity is key: within a job title, jobs with 10% higher wages do attract 7.7% more applicants. Furthermore, our findings are consistent with a higher return to worker quality for hires in “manager” and “senior” job titles. Overall, our findings demonstrate the power of words in the matching process.},
  author    = {Ioana Marinescu and Ronald Wolthoff},
  doi       = {10.1086/705903},
  issue     = {2},
  journal   = {Journal of Labor Economics},
  month     = {4},
  pages     = {535-568},
  publisher = {University of Chicago Press},
  title     = {Opening the black box of the matching function: The power of words},
  volume    = {38},
  year      = {2020}
}
@article{Slamet018,
  abstract  = {Many organisations (government of non-government) use websites to share information of new recruitment for the workers. This information overflows on thousands of sites with various attributes and criteria. However, this availability forms a complex puzzle in the selection process and lead to inefficient runtime. This study proposes a simple method for job searching simplification through a construction and collaboration of web scraping technique and classification using Naïve Bayes on search engine. This study is resulting an effective and efficient application for users to seek a potential job that fit in with their interests.},
  author    = {C Slamet and R Andrian and D S Maylawati and Suhendar and W Darmalaksana and M A Ramdhani},
  issue     = {1},
  journal   = {IOP Conference Series: Materials Science and Engineering},
  month     = {1},
  pages     = {012038},
  publisher = {IOP Publishing},
  title     = {Web Scraping and Naïve Bayes Classification for Job Search Engine},
  volume    = {288},
  url       = {https://iopscience.iop.org/article/10.1088/1757-899X/288/1/012038 https://iopscience.iop.org/article/10.1088/1757-899X/288/1/012038/meta},
  year      = {2018}
}
@article{Uter2020,
  abstract  = {For the epidemiology of work-related skin diseases, comprising surveillance and analytical approaches, occupational exposure can be represented by the occupation (job title) or the industry a person is working in. For both concepts, several international and national classification systems exist, which are revised from time to time. As global classification of occupations, the “International Standard Classification of Occupations” (ISCO, presently ISCO-08) of the International Labour Organisation is available. The “International Standard Industrial Classification of all Economic Activities” (ISIC), presently in its fourth revision, issued by the United Nations Statistics Division, is globally used to document industrial sectors; however, in several countries national classification systems are (also) used. All such classifications have been developed to serve broad administrative and research purposes. For this reason, it is often necessary to adapt a classification of occupation or industry to medical needs. To this end, a more refined subcategorization, respecting, however, the hierarchical structure of the classification it is based upon, and general rules for the construction of classification systems, can be employed to achieve a best possible degree of detail. Conversely, some other categories (e.g., different types of “office work”) deemed largely identical with regard to relevant exposures may be aggregated. The aim is to achieve a clinically manageable, reliable catalogue and consistent usage aided by clear definitions.},
  author    = {Wolfgang Uter},
  doi       = {10.1007/978-3-319-68617-2_7},
  journal   = {Kanerva's Occupational Dermatology},
  keywords  = {Classification,Epidemiology,Exposure,ILO,ISCO,ISIC,Industry,Job exposure matrix,NACE,NAICS,Occupation,Surveillance},
  month     = {1},
  pages     = {61-67},
  publisher = {Springer, Cham},
  title     = {Classification of Occupations},
  url       = {https://link.springer.com/referenceworkentry/10.1007/978-3-319-68617-2_7},
  year      = {2020}
}
@article{WangJ2019,
  abstract  = {In online recruitment, job title classification is a fundamental task that enables several downstream applications like job recommendation and ranking for job search. A special case of multi-class text classification, the job title classification problem takes as input two components from a job posting -a short job title and a lengthier job description, and normalizes the raw job title into its closest match from the given taxonomy. Typically, the job title, though shorter in length, contains more targeted signals than the job description, that can contain additional information irrelevant to the context. On the other hand, the job description often provides valuable information that helps steer the classification model towards choosing the best match. Achieving a balance between the two components is not a trivial task. In this paper, we propose a multi-stream CNN based model for job title classification, that learns semantic features on both character and word level. We collected about 15 million data points from one of the largest online job boards, Careerbuilder, to train the model. Due to the universal problem of getting massive labeled data, we adopt a weakly supervised method to efficiently generate noisy labels for this large data set. Compared with the current state-of-the-art job title classification systems, the proposed model, DeepCarotene, shows a significant improvement in performance. This model provides a new direction of CNN based end-to-end approach for job title classification.},
  author    = {Jingya Wang and Kareem Abdelfatah and Mohammed Korayem and Janani Balaji},
  doi       = {10.1109/BIGDATA47090.2019.9005673},
  journal   = {Proceedings - 2019 IEEE International Conference on Big Data, Big Data 2019},
  month     = {12},
  pages     = {1953-1961},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  title     = {DeepCarotene -Job Title Classification with Multi-stream Convolutional Neural Network},
  year      = {2019}
}
@article{Zhu2017,
  abstract = {Automatic and accurate classification of items enables numerous downstream applications in many domains. These applications can range from faceted browsing of items to product recommendations and big data analytics. In the online recruitment domain, we refer to classifying job ads to a predefined occupation taxonomy as job title classification. A large-scale job title classification system can power various downstream applications such as query expansion, semantic search, job recommendations and labor market analytics. Such classification systems mostly use Bag-of-Words (BOW) model for document representation and consider only the job titles when classifying job ads. However the BOW model lacks the semantic discrimination capability that is needed to accurately classify job ads when they contain multiple aspects of the job such as the job description, job requirements, company overview and other details. In this paper we explore the applicability of recent advances in the word and document embedding space to the problem of job title classification. We investigate several document embedding approaches and propose a novel customized document embedding strategy for job title classification that addresses the multi-aspect job ad issue. Our experimental results show that incorporating document embedding approaches in a job title classification system improves the classification accuracy on entire job ads compared to approaches based on the BOW model.},
  author   = {Yun Zhu and Faizan Javed and Ozgur Ozturk},
  journal  = {The Thirtieth International Flairs Conference},
  keywords = {Doc2Vec,Job Title Classification,Word2Vec},
  month    = {5},
  title    = {Document Embedding Strategies for Job Title Classification},
  url      = {https://www.aaai.org/ocs/index.php/FLAIRS/FLAIRS17/paper/view/15470},
  year     = {2017}
}
@article{Javed2015,
  abstract  = {In the online job recruitment domain, accurate classification of jobs and resumes to occupation categories is important for matching job seekers with relevant jobs. An example of such a job title classification system is an automatic text document classification system that utilizes machine learning. Machine learning-based document classification techniques for images, text and related entities have been well researched in academia and have also been successfully applied in many industrial settings. In this paper we present Carotene, a machine learning-based semi-supervised job title classification system that is currently in production at CareerBuilder. Carotene leverages a varied collection of classification and clustering tools and techniques to tackle the challenges of designing a scalable classification system for a large taxonomy of job categories. It encompasses these techniques in a cascade classifier architecture. We first present the architecture of Carotene, which consists of a two-stage coarse and fine level classifier cascade. We compare Carotene to an early version that was based on a flat classifier architecture and also compare and contrast Carotene with a third party occupation classification system. The paper concludes by presenting experimental results on real world industrial data using both machine learning metrics and actual user experience surveys.},
  author    = {Faizan Javed and Qinlong Luo and Matt McNair and Ferosh Jacob and Meng Zhao and Tae Seung Kang},
  doi       = {10.1109/BIGDATASERVICE.2015.61},
  journal   = {Proceedings - 2015 IEEE 1st International Conference on Big Data Computing Service and Applications, BigDataService 2015},
  keywords  = {job title classification,machine learning,text classification},
  month     = {8},
  pages     = {286-293},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  title     = {Carotene: A job title classification system for the online recruitment domain},
  year      = {2015}
}
@article{Tomar2015,
  abstract  = {Least Squares Twin Support Vector Machine (LSTSVM) is a binary classifier and the extension of it to multiclass is still an ongoing research issue. In this paper, we extended the formulation of binary LSTSVM classifier to multi-class by using the concepts such as "One-versus-All", "One-versus-One", "All-versus-One" and Directed Acyclic Graph (DAG). This paper performs a comparative analysis of these multi-classifiers in terms of their advantages, disadvantages and computational complexity. The performance of all the four proposed classifiers has been validated on twelve benchmark datasets by using predictive accuracy and training-testing time. All the proposed multi-classifiers have shown better performance as compared to the typical multi-classifiers based on 'Support Vector Machine' and 'Twin Support Vector Machine'. Friedman's statistic and Nemenyi post hoc tests are also used to test significance of predictive accuracy differences between classifiers.},
  author    = {Divya Tomar and Sonali Agarwal},
  journal   = {Knowledge-Based Systems},
  keywords  = {Least squares twin support vector machine,Multi-class classification,Multi-class least squares twin support,Support vector machine,Twin support vector machine,vector machine},
  month     = {6},
  pages     = {131-147},
  publisher = {Elsevier},
  title     = {A comparison on multi-class classification methods based on least squares twin support vector machine},
  volume    = {81},
  year      = {2015}
}
@article{Guo2015,
  abstract  = {Traditional multi-class classification models are based on labeled data and are not applicable to unlabeled data. To overcome this limitation, this paper presents a multi-class classification model that is based on active learning and support vector machines (MC-SVMA), which can be used to address unlabeled data. Firstly, a number of unlabeled samples are selected as the most valuable samples using the active learning technique. And then, the model quickly mines the pattern classes for unlabeled samples by computing the differences between the unlabeled and labeled samples. Moreover, to label the unlabeled samples accurately and acquire more class information, the active learning strategy is also used to select compatible, rejected and uncertain samples, which are labeled by experts. Thus, the proposed model can determine as many classes as possible while requiring fewer samples to be manually labeled. This approach permits an unlabeled multi-classification problem to be translated into a classical supervised multi-classification problem. The experimental results demonstrate that the MC-SVMA model is efficient and exhibits good generalization performance.},
  author    = {Husheng Guo and Wenjian Wang},
  doi       = {10.1016/J.PATCOG.2014.12.009},
  issue     = {5},
  journal   = {Pattern Recognition},
  keywords  = {Active learning,MC-SVMA model,Multi-class classification with unknown categories,Support vector machine},
  month     = {5},
  pages     = {1577-1597},
  publisher = {Pergamon},
  title     = {An active learning-based SVM multi-class classification model},
  volume    = {48},
  year      = {2015}
}
@article{Farooq2017,
  abstract  = {In the recent years, deep learning has gained huge fame in solving problems from various fields including medical image analysis. This work proposes a deep convolutional neural network based pipeline for the diagnosis of Alzheimer's disease and its stages using magnetic resonance imaging (MRI) scans. Alzheimer's disease causes permanent damage to the brain cells associated with memory and thinking skills. The diagnosis of Alzheimer's in elderly people is quite difficult and requires a highly discriminative feature representation for classification due to similar brain patterns and pixel intensities. Deep learning techniques are capable of learning such representations from data. In this paper, a 4-way classifier is implemented to classify Alzheimer's (AD), mild cognitive impairment (MCI), late mild cognitive impairment (LMCI) and healthy persons. Experiments are performed using ADNI dataset on a high performance graphical processing unit based system and new state-of-The-Art results are obtained for multiclass classification of the disease. The proposed technique results in a prediction accuracy of 98.8%, which is a noticeable increase in accuracy as compared to the previous studies and clearly reveals the effectiveness of the proposed method.},
  author    = {Ammarah Farooq and Syedmuhammad Anwar and Muhammad Awais and Saad Rehman},
  doi       = {10.1109/IST.2017.8261460},
  journal   = {IST 2017 - IEEE International Conference on Imaging Systems and Techniques, Proceedings},
  keywords  = {Alzheimer's disease,MRI,deep learning,multiclass},
  month     = {7},
  pages     = {1-6},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  title     = {A deep CNN based multi-class classification of Alzheimer's disease using MRI},
  volume    = {2018-January},
  year      = {2017}
}
@article{Tang2019,
  abstract  = {Although support vector machine (SVM) and its variants have been combined successfully with partitioning strategies for multiclass classification, a series of individual classifiers have to be considered separately, which significantly limits the sparseness of the solution. In this work, from an absolutely novel perspective, we proposed a regular simplex support vector machine (RSSVM) for the K-class classification. RSSVM maps the K classes to K vertices of a (K−1)-dimensional regular simplex so that the K-class classification becomes a (K−1)-output learning task. We measured the training loss by comparing the square distances between the output point of each sample and the vertices. As a result, we were able to implement RSSVM by constructing a single primal problem with linear inequality constraints, endowing it with excellent sparseness. This method, however, made the computational complexity of RSSVM higher than commonly used K-class classification methods. To overcome this drawback, we added an appropriate regularized term to the primal problem, making the dual problem a quadratic programming problem, and we developed an exclusive sequential minimization optimization–type solver to accelerate our ability to solve it. Numerical experiments indicated that our proposed RSSVM not only demonstrated excellent sparseness but also superior overall accuracy, efficiency, and scalability.},
  author    = {Long Tang and Yingjie Tian and Panos M. Pardalos},
  journal   = {Information Sciences},
  keywords  = {K-class classification,Regular simplex support vector machine,Sparseness,Support vector machine},
  month     = {4},
  pages     = {324-338},
  publisher = {Elsevier},
  title     = {A novel perspective on multiclass classification: Regular simplex support vector machine},
  volume    = {480},
  year      = {2019}
}
@article{Wu2004,
  abstract = {Pairwise coupling is a popular multi-class classification method that combines all comparisons for each pair of classes. This paper presents two approaches for obtaining class probabilities. Both methods can be reduced to linear systems and are easy to implement. We show conceptually and experimentally that the proposed approaches are more stable than the two existing popular methods: voting and the method by Hastie and Tibshirani (1998).},
  author   = {Ting-Fan Wu and Chih-Jen Lin and Ruby C Weng},
  journal  = {Journal of Machine Learning Research},
  keywords = {pairwise coupling,probability estimates,random forest,support vector ma-chines},
  pages    = {975-1005},
  title    = {Probability Estimates for Multi-class Classification by Pairwise Coupling},
  volume   = {5},
  year     = {2004}
}
@article{Li2004,
  abstract  = {Summary: This paper studies the problem of building multiclass classifiers for tissue classification based on gene expression. The recent development of microarray technologies has enabled biologists to quantify gene expression of tens of thousands of genes in a single experiment. Biologists have begun collecting gene expression for a large number of samples. One of the urgent issues in the use of microarray data is to develop methods for characterizing samples based on their gene expression. The most basic step in the research direction is binary sample classification, which has been studied extensively over the past few years. This paper investigates the next step - multiclass classification of samples based on gene expression. The characteristics of expression data (e.g. large number of genes with small sample size) makes the classification problem more challenging. The process of building multiclass classifiers is divided into two components: (i) selection of the features (i.e. genes) to be used for training and testing and (ii) selection of the classification method. This paper compares various feature selection methods as well as various state-of-the-art classification methods on various multiclass gene expression datasets. Our study indicates that multiclass classification problem is much more difficult than the binary one for the gene expression datasets. The difficulty lies in the fact that the data are of high dimensionality and that the sample size is small. The classification accuracy appears to degrade very rapidly as the number of classes increases. In particular, the accuracy was very low regardless of the choices of the methods for large-class datasets (e.g. NCI60 and GCM). While increasing the number of samples is a plausible solution to the problem of accuracy degradation, it is important to develop algorithms that are able to analyze effectively multiple-class expression data for these special datasets. © Oxford University Press 2004; all rights reserved.},
  author    = {Tao Li and Chengliang Zhang and Mitsunori Ogihara},
  issue     = {15},
  journal   = {Bioinformatics},
  month     = {10},
  pages     = {2429-2437},
  publisher = {Oxford Academic},
  title     = {A comparative study of feature selection and multiclass classification methods for tissue classification based on gene expression},
  volume    = {20},
  url       = {https://academic.oup.com/bioinformatics/article/20/15/2429/233509},
  year      = {2004}
}
@article{Har-Peled2002,
  abstract  = {In this paper, we present a newviewof multiclass classification and introduce the constraint classification problem, a generalization that captures many flavors of multiclass classification. We provide the first optimal, distribution independent bounds for many multiclass learning algorithms, including winner-take-all (WTA). Based on our view, we present a learning algorithm that learns via a single linear classifier in high dimension. In addition to the distribution independent bounds, we provide a simple margin-based analysis improving generalization bounds for linear multiclass support vector machines. © Springer-Verlag Berlin Heidelberg 2002.},
  author    = {Sariel Har-Peled and Dan Roth and Dav Zimak},
  doi       = {10.1007/3-540-36169-3_29},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  month     = {11},
  pages     = {365-379},
  publisher = {Springer, Berlin, Heidelberg},
  title     = {Constraint Classification: A New Approach to Multiclass Classification},
  volume    = {2533},
  url       = {https://link.springer.com/chapter/10.1007/3-540-36169-3_29},
  year      = {2002}
}
@article{Tax2002,
  abstract = {The generalization from two-class classification to multiclass classification is not straightforward for discriminants which are not based on density estimation. Simple combining methods use voting, but this has the drawback of inconsequent labelings and ties. More advanced methods map the discriminant outputs to approximate posterior probability estimates and combine these, while other methods use error-correcting output codes. In this paper we want to show the possibilities of simple generalizations of the two-class classification, using voting and combinations of approximate posterior probabilities. © 2002 IEEE.},
  author   = {David M.J. Tax and Robert P.W. Duin},
  doi      = {10.1109/ICPR.2002.1048253},
  issue    = {2},
  journal  = {Proceedings - International Conference on Pattern Recognition},
  pages    = {124-127},
  title    = {Using two-class classifiers for multiclass classification},
  volume   = {16},
  year     = {2002}
}
@article{Edu2007,
  abstract = {Binary classification is a well studied special case of the classification problem. Statistical properties of binary classifiers, such as consistency, have been investigated in a variety of settings. Binary classification methods can be generalized in many ways to handle multiple classes. It turns out that one can lose consistency in generalizing a binary classification method to deal with multiple classes. We study a rich family of multiclass methods and provide a necessary and sufficient condition for their consistency. We illustrate our approach by applying it to some multiclass methods proposed in the literature.},
  author   = {Ambuj@cs Berkeley Edu and Peter L Bartlett and Bartlett@cs Berkeley Edu},
  journal  = {Journal of Machine Learning Research},
  keywords = {Bayes risk,consistency,multiclass classification},
  pages    = {1007-1025},
  title    = {On the Consistency of Multiclass Classification Methods Ambuj Tewari},
  volume   = {8},
  year     = {2007}
}
@article{Huang2012,
  abstract = {Due to the simplicity of their implementations, least square support vector machine (LS-SVM) and proximal support vector machine (PSVM) have been widely used in binary classification applications. The conventional LS-SVM and PSVM cannot be used in regression and multiclass classification applications directly, although variants of LS-SVM and PSVM have been proposed to handle such cases. This paper shows that both LS-SVM and PSVM can be simplified further and a unified learning framework of LS-SVM, PSVM, and other regularization algorithms referred to extreme learning machine (ELM) can be built. ELM works for the "generalized" single-hidden-layer feedforward networks (SLFNs), but the hidden layer (or called feature mapping) in ELM need not be tuned. Such SLFNs include but are not limited to SVM, polynomial network, and the conventional feedforward neural networks. This paper shows the following: 1) ELM provides a unified learning platform with a widespread type of feature mappings and can be applied in regression and multiclass classification applications directly; 2) from the optimization method point of view, ELM has milder optimization constraints compared to LS-SVM and PSVM; 3) in theory, compared to ELM, LS-SVM and PSVM achieve suboptimal solutions and require higher computational complexity; and 4) in theory, ELM can approximate any target continuous function and classify any disjoint regions. As verified by the simulation results, ELM tends to have better scalability and achieve similar (for regression and binary class cases) or much better (for multiclass cases) generalization performance at much faster learning speed (up to thousands times) than traditional SVM and LS-SVM. © 2006 IEEE.},
  author   = {Guang Bin Huang and Hongming Zhou and Xiaojian Ding and Rui Zhang},
  doi      = {10.1109/TSMCB.2011.2168604},
  issue    = {2},
  journal  = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
  keywords = {Extreme learning machine (ELM),feature mapping,kernel,least square support vector machine (LS-SVM),proximal support vector machine (PSVM),regularization network},
  month    = {4},
  pages    = {513-529},
  title    = {Extreme learning machine for regression and multiclass classification},
  volume   = {42},
  year     = {2012}
}
@article{Aly2005,
  abstract = {Supervised classification algorithms aim at producing a learning model from a labeled training set. Various successful techniques have been proposed to solve the problem in the binary classification case. The multi-class classification case is more delicate, as many of the algorithms were introduced basically to solve binary classification problems. In this short survey we investigate the various techniques for solving the multiclass classification problem.},
  author   = {Mohamed Aly},
  journal  = {Neural Netw },
  pages    = {1-9},
  title    = {Survey on Multiclass Classification Methods},
  volume   = {19},
  year     = {2005}
}
@article{Kim2014,
  abstract  = {Short-text classification is increasingly used in a wide range of applications. However, it still remains a challenging problem due to the insufficient nature of word occurrences in short-text documents, although some recently developed methods which exploit syntactic or semantic information have enhanced performance in short-text classification. The language-dependency problem, however, caused by the heavy use of grammatical tags and lexical databases, is considered the major drawback of the previous methods when they are applied to applications in diverse languages. In this article, we propose a novel kernel, called language independent semantic (LIS) kernel, which is able to effectively compute the similarity between short-text documents without using grammatical tags and lexical databases. From the experiment results on English and Korean datasets, it is shown that the LIS kernel has better performance than several existing kernels. © 2013 Elsevier Ltd. All rights reserved.},
  author    = {Kwanho Kim and Beom Suk Chung and Yerim Choi and Seungjun Lee and Jae Yoon Jung and Jonghun Park},
  issue     = {2},
  journal   = {Expert Systems with Applications},
  keywords  = {Kernel method,Language independent semantic kernel,Short-text document classification,Similarity measure},
  month     = {2},
  note      = {Auf Seite direkt durchlesen, kann man nicht als PDF laden <br/><br/>},
  pages     = {735-743},
  publisher = {Pergamon},
  title     = {Language independent semantic kernels for short-text classification},
  volume    = {41},
  year      = {2014}
}
@article{WangY2017,
  author    = {Ye Wang and Zhi Zhou and Shan Jin and Debin Liu and Mi Lu},
  issue     = {1},
  journal   = {IOP Conference Series: Materials Science and Engineering},
  publisher = {Institute of Physics Publishing},
  title     = {Comparisons and Selections of Features and Classifiers for Short Text Classification},
  volume    = {261},
  year      = {2017},
  pages     = {1-8}
}
@article{Khamar2013,
  author  = {Khushbu Khamar},
  issue   = {4},
  journal = {International Journal of Advanced Research in Computer and Communication Engineering},
  pages   = {1916-1919},
  title   = {Short Text Classification Using kNN Based on Distance Function},
  volume  = {2},
  year    = {2013}
}
@article{WangF2014,
  author    = {Fang Wang and Zhongyuan Wang and Zhoujun Li and Ji Rong Wen},
  journal   = {CIKM 2014 - Proceedings of the 2014 ACM International Conference on Information and Knowledge Management},
  month     = {11},
  pages     = {1069-1078},
  publisher = {Association for Computing Machinery, Inc},
  title     = {Concept-based short text classification and ranking},
  year      = {2014}
}
@article{Chen2019,
  author    = {Jindong Chen and Yizhou Hu and Jingping Liu and Yanghua Xiao and Haiyun Jiang},
  journal   = {33rd AAAI Conference on Artificial Intelligence, AAAI 2019},
  pages     = {6252-6259},
  publisher = {AAAI Press},
  title     = {Deep Short Text Classification with Knowledge Powered Attention},
  year      = {2019}
}
@article{Javed2016,
  author  = {Faizan Javed and Matt McNair and Ferosh Jacob and Meng Zhao},
  journal = {arXiv preprint arXiv},
  month   = {6},
  pages   = {1-4},
  title   = {Towards a Job Title Classification System},
  url     = {https://arxiv.org/abs/1606.00917v1},
  year    = {2016}
}
@article{Malherbe2014,
  abstract  = {Nowadays, in the Web 2.0 reality, one of the most challenging task for companies that aim to manage and recommend job offers is to convey this enormous amount of information in a succinct and intelligent manner such to increase the performances of matching operations against users profiles/curricula and optimize the time/space complexity of these processes. With this goal, this paper presents a novel method to formalize the textual content of job offers that aims at identifying the most relevant information and fields expressed by them and leverage this compact formalization for job recommendation and profile matching in social network environments. This method has been then developed and tested in the industrial environment represented by Multiposting and Work4, world leaders in digital solutions of e-recruitment problems.},
  author    = {Emmanuel Malherbe and Mamadou Diaby and Mario Cataldi and Emmanuel Viennet and Marie Aude Aufaure},
  doi       = {10.1109/ASONAM.2014.6921646},
  journal   = {ASONAM 2014 - Proceedings of the 2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
  keywords  = {Facebook,Job categorization,Job recommendation,LinkedIn,SVM,field selection},
  month     = {10},
  pages     = {588-595},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  title     = {Field selection for job categorization and recommendation to social network users},
  year      = {2014}
}

@article{Yang1999,
  abstract  = {This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance; except for a Naive Bayes approach, the other learning algorithms also performed relatively well.},
  author    = {Yiming Yang},
  issue     = {1},
  journal   = {Information Retrieval 1999 1:1},
  keywords  = {Data Mining and Knowledge Discovery,Data Structures and Information Theory,Information Storage and Retrieval,Natural Language Processing (NLP),Pattern Recognition},
  pages     = {69-90},
  publisher = {Springer},
  title     = {An Evaluation of Statistical Approaches to Text Categorization},
  volume    = {1},
  url       = {https://link.springer.com/article/10.1023/A:1009982220290},
  year      = {1999}
}
@article{Yin2017,
  abstract = {Deep neural networks (DNN) have revolutionized the field of natural language
              processing (NLP). Convolutional neural network (CNN) and recurrent neural
              network (RNN), the two main types of DNN architectures, are widely explored to
              handle various NLP tasks. CNN is supposed to be good at extracting
              position-invariant features and RNN at modeling units in sequence. The state of
              the art on many NLP tasks often switches due to the battle between CNNs and
              RNNs. This work is the first systematic comparison of CNN and RNN on a wide
              range of representative NLP tasks, aiming to give basic guidance for DNN
              selection.},
  author   = {Wenpeng Yin and Katharina Kann and Mo Yu and Hinrich Schütze},
  journal  = {arXiv preprint arXiv},
  month    = {2},
  pages    = {1-7},
  title    = {Comparative Study of CNN and RNN for Natural Language Processing},
  url      = {https://arxiv.org/abs/1702.01923v1},
  year     = {2017}
}
@article{Lewis1994,
  abstract = {This paper examines the use of inductive learning to categorize natural language documents into predeened content categories. Categorization of text is of increasing importance in information retrieval and natural language processing systems. Previous research on automated text categorization has mixed machine learning and knowledge engineering methods, making it diicult to draw conclusions about the performance of particular methods. In this paper we present empirical results on the performance of a Bayesian classiier and a decision tree learning algorithm on two text categorization data sets. We nd that both algorithms achieve reasonable performance and allow controlled tradeoos between false positives and false negatives. The stepwise feature selection in the decision tree algorithm is particularly eeective in dealing with the large feature sets common in text categorization. However, even this algorithm is aided by an initial preeltering of features, connrming the results found by Almuallim and Dietterich on artiicial data sets. We also demonstrate the impact of the time-varying nature of category deenitions.},
  author   = {David D Lewis and Marc Ringuette},
  journal  = {Third annual symposium on document analysis and information retrieval.},
  pages    = {1-14},
  title    = {A Comparison of Two Learning Algorithms for Text Categorization (Symposium on Document Analysis and IR, ISRI, Las Vegas)},
  volume   = {33},
  year     = {1994}
}
@article{Korde2012,
  abstract = {As most information (over 80%) is stored as text, text mining is believed to have a high commercial potential value. knowledge may be discovered from many sources of information; yet, unstructured texts remain the largest readily available source of knowledge .Text classification which classifies the documents according to predefined categories .In this paper we are tried to give the introduction of text classification, process of text classification as well as the overview of the classifiers and tried to compare the some existing classifier on basis of few criteria like time complexity, principal and performance.},
  author   = {Vandana Korde and C Namrata Mahender},
  doi      = {10.5121/ijaia.2012.3208},
  issue    = {2},
  journal  = {International Journal of Artificial Intelligence & Applications (IJAIA)},
  keywords = {Classifiers,Text Representation,Text classification},
  pages    = {85-99},
  title    = {Text classification and classifiers: a survey},
  volume   = {3},
  year     = {2012}
}
@article{Uysal2014,
  abstract  = {Preprocessing is one of the key components in a typical text classification framework. This paper aims to extensively examine the impact of preprocessing on text classification in terms of various aspects such as classification accuracy, text domain, text language, and dimension reduction. For this purpose, all possible combinations of widely used preprocessing tasks are comparatively evaluated on two different domains, namely e-mail and news, and in two different languages, namely Turkish and English. In this way, contribution of the preprocessing tasks to classification success at various feature dimensions, possible interactions among these tasks, and also dependency of these tasks to the respective languages and domains are comprehensively assessed. Experimental analysis on benchmark datasets reveals that choosing appropriate combinations of preprocessing tasks, rather than enabling or disabling them all, may provide significant improvement on classification accuracy depending on the domain and language studied on. © 2013 Elsevier Ltd. All rights reserved.},
  author    = {Alper Kursat Uysal and Serkan Gunal},
  issue     = {1},
  journal   = {Information Processing & Management},
  keywords  = {Pattern recognition,Text categorization,Text classification,Text preprocessing},
  month     = {1},
  pages     = {104-112},
  publisher = {Pergamon},
  title     = {The impact of preprocessing on text classification},
  volume    = {50},
  year      = {2014}
}
@article{Ikonomakis2005,
  abstract = {Automated text classification has been considered as a vital method to manage and process a vast amount of documents in digital forms that are widespread and continuously increasing. In general, text classification plays an important role in information extraction and summarization, text retrieval, and question-answering. This paper illustrates the text classification process using machine learning techniques. The references cited cover the major theoretical issues and guide the researcher to interesting research directions.},
  author   = {M Ikonomakis and S Kotsiantis and V Tampakas},
  issue    = {8},
  journal  = {WSEAS TRANSACTIONS on COMPUTERS },
  keywords = {Key-Words: text mining,feature selection,learning algorithms,text representation},
  pages    = {966-974},
  title    = {Text Classification Using Machine Learning Techniques},
  volume   = {4},
  year     = {2005}
}
@article{Schapire2000,
  abstract = {This work focuses on algorithms which learn from examples to perform multiclass text and speech categorization tasks. Our approach is based on a new and improved family of boosting algorithms. We describe in detail an implementation, called BoosTexter, of the new boosting algorithms for text categorization tasks. We present results comparing the performance of BoosTexter and a number of other text-categorization algorithms on a variety of tasks. We conclude by describing the application of our system to automatic call-type identification from unconstrained spoken customer responses.},
  author   = {Robert E. Schapire and Yoram Singer},
  journal  = {Machine Learning},
  pages    = {135-168},
  title    = {BoosTexter: A Boosting-based System for Text Categorization},
  volume   = {39},
  year     = {2000}
}
@article{Yang1997,
  abstract = {This paper is a comparative study of feature selection methods in statistical learning of text categorization. The focus is on aggressive dimensionality reduction. Five methods were evaluated, including term selection based on document frequency (DF), information gain (IG), mutual information (MI), a 2-test (CHI), and term strength (TS). We found IG and CHI most eeective in our experiments. Using IG thresholding with a k-nearest neighbor classiier on the Reuters corpus , removal of up to 98% removal of unique terms actually yielded an improved classii-cation accuracy (measured by average precision). DF thresholding performed similarly. Indeed we found strong correlations between the DF, IG and CHI values of a term. This suggests that DF thresholding, the simplest method with the lowest cost in computation, can be reliably used instead of IG or CHI when the computation of these measures are too expensive. TS compares favorably with the other methods with up to 50% vocabulary reduction but is not competitive at higher vocabulary reduction levels. In contrast, MI had relatively poor performance due to its bias towards favoring rare terms, and its sensitivity to probability estimation errors.},
  author   = {Yiming Yang and Jan O Pedersen},
  journal  = {Icml},
  pages    = {412-420},
  title    = {A Comparative Study on Feature Selection in Text Categorization},
  volume   = {97},
  year     = {1997}
}
@article{Sebastiani2001,
  author    = {Fabrizio Sebastiani},
  issue     = {1},
  journal   = {ACM Computing Surveys},
  month     = {10},
  pages     = {1-47},
  publisher = {Association for Computing Machinery (ACM)},
  title     = {Machine Learning in Automated Text Categorization},
  volume    = {34},
  year      = {2001}
}
@article{Song2014,
  author    = {Ge Song and Yunming Ye and Xiaolin Du and Xiaohui Huang and Shifu Bie},
  issue     = {5},
  journal   = {Journal of Multimedia},
  publisher = {Academy Publisher},
  title     = {Short Text Classification: A Survey},
  volume    = {9},
  year      = {2014},
  pages     = {635-643}
}

@article{Do2005,
  abstract = {Linear text classification algorithms work by computing an inner product between a test document vector and a parameter vector. In many such algorithms, including naive Bayes and most TFIDF variants, the parameters are determined by some simple, closed-form, function of training set statistics; we call this mapping mapping from statistics to parameters, the parameter function. Much research in text classification over the last few decades has consisted of manual efforts to identify better parameter functions. In this paper, we propose an algorithm for automatically learning this function from related classification problems. The parameter function found by our algorithm then defines a new learning algorithm for text classification, which we can apply to novel classification tasks. We find that our learned classifier outperforms existing methods on a variety of multiclass text classification tasks.},
  author   = {Chuong B Do and Andrew Y Ng},
  journal  = {Advances in Neural Information Processing Sysems },
  pages    = {299-306},
  title    = {Transfer learning for text classification},
  volume   = {18},
  year     = {2005}
}
@article{Kowsari2019,
  author    = {Kamran Kowsari and Kiana Jafari Meimandi and Mojtaba Heidarysafa and Sanjana Mendu and Laura Barnes and Donald Brown},
  issue     = {4},
  journal   = {Information 2019},
  month     = {4},
  pages     = {150},
  publisher = {Multidisciplinary Digital Publishing Institute},
  title     = {Text Classification Algorithms: A Survey},
  volume    = {10},
  year      = {2019}
}
@article{Colas2006,
  author    = {Fabrice Colas and Pavel Brazdil},
  journal   = {IFIP International Federation for Information Processing},
  pages     = {169-178},
  publisher = {Springer, Boston, MA},
  title     = {Comparison of SVM and Some Older Classification Algorithms in Text Classification Tasks},
  volume    = {217},
  year      = {2006}
}
@inproceedings{Vijayan2017,
  author    = {Vikas K. Vijayan and K. R. Bindu and Latha Parameswaran},
  journal   = {2017 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2017},
  pages     = {1109-1113},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  title     = {A comprehensive study of text classification algorithms},
  year      = {2017}
}
